{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"mujtabamatin/air-quality-and-pollution-assessment\")\n",
    "print(f\"Path to dataset files: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "dataset_path = Path(f\"{path}\").joinpath(\"updated_pollution_dataset.csv\")\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.rename(columns={\"Air Quality\": \"y\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.countplot(x=\"y\", data=dataset)\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x() + p.get_width() / 2., height + 20, height + 20, ha=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "dataset[\"y\"] = encoder.fit_transform(dataset[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle as cp\n",
    "\n",
    "run = 1\n",
    "root_path = Path(f\"./saved-runs/run-{run}\")\n",
    "root_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "encoder_file = root_path.joinpath(\"encoders\", \"label_encoder.pkl\")\n",
    "encoder_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(encoder_file, \"wb\") as f:\n",
    "    cp.dump(encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "dataset[[\n",
    "    'Temperature', 'Humidity', 'PM2.5', 'PM10', 'NO2', 'SO2', 'CO', 'Proximity_to_Industrial_Areas',\n",
    "    'Population_Density'\n",
    " ]] = scaler.fit_transform(dataset[[\n",
    "    'Temperature', 'Humidity', 'PM2.5', 'PM10', 'NO2', 'SO2', 'CO', 'Proximity_to_Industrial_Areas',\n",
    "    'Population_Density'\n",
    " ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_file = root_path.joinpath(\"scalers\", \"standard_scaler.pkl\")\n",
    "encoder_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(encoder_file, \"wb\") as f:\n",
    "    cp.dump(encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(10, 5), dpi=100)\n",
    "\n",
    "corr_map = dataset.corr()[\"y\"]\n",
    "ax = sns.barplot(corr_map)\n",
    "for p in ax.patches:\n",
    "    height = round(p.get_height(), 4)\n",
    "    ax.text(p.get_x() + p.get_width() / 2., height, height, ha=\"center\")\n",
    "\n",
    "_ = plt.xticks(rotation=45, ha=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(dataset.shape[1]//3, 3, figsize=(15, 10), dpi=100)\n",
    "\n",
    "col = 0\n",
    "row = 0\n",
    "for column in dataset.columns:\n",
    "    if column == \"y\":\n",
    "        continue\n",
    "    sns.histplot(x=dataset[column], kde=True, ax=axes[col, row])\n",
    "    plt.title(column)\n",
    "\n",
    "    col, row = (col + 1, 0) if row >= 2 else (col, row + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "columns_to_transform = [\n",
    "    # \"PM2.5\", \"PM10\", \"CO\", \"Proximity_to_Industrial_Areas\"\n",
    "    \"PM2.5\", \"PM10\"\n",
    "]\n",
    "transformer = PowerTransformer(method=\"yeo-johnson\", standardize=False)\n",
    "dataset[columns_to_transform] = transformer.fit_transform(dataset[columns_to_transform])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(dataset.shape[1]//3, 3, figsize=(15, 10), dpi=100)\n",
    "\n",
    "col = 0\n",
    "row = 0\n",
    "for column in dataset.columns:\n",
    "    if column == \"y\":\n",
    "        continue\n",
    "    sns.histplot(x=dataset[column], kde=True, ax=axes[col, row])\n",
    "    plt.title(column)\n",
    "\n",
    "    col, row = (col + 1, 0) if row >= 2 else (col, row + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_file = root_path.joinpath(\"transformer\", \"yeo_johnson_transformer.pkl\")\n",
    "transformer_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(transformer_file, \"wb\") as f:\n",
    "    cp.dump(transformer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataset.drop(columns=[\"y\"])\n",
    "y = dataset[[\"y\"]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {X_train.shape[0]}\")\n",
    "print(f\"Test dataset size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=30,\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    alpha=0.003,\n",
    "    batch_size=64,\n",
    "    learning_rate=\"constant\",\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=200,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    tol=0.001,\n",
    "    verbose=True,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.2,\n",
    "    n_iter_no_change=20,\n",
    ")\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(model.loss_curve_, legend=True)\n",
    "plt.title(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(model.validation_scores_)\n",
    "plt.title(\"Validation accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix(y_test, model.predict(X_test)),\n",
    "    display_labels=encoder.inverse_transform(model.classes_)\n",
    ").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = root_path.joinpath(\"saved-model\", \"model.pkl\")\n",
    "model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(model_path, \"wb\") as f:\n",
    "    cp.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copyfile(\n",
    "    \"./air-quality-pollution.ipynb\",\n",
    "    root_path.joinpath(\"air-quality-pollution.ipynb\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-practice-ml-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
