{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e1054ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cudf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "114815c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = cudf.read_csv('./Loan_Default.csv')\n",
    "\n",
    "# Setting the max_columns to be displayed with pandas (here with cudf)\n",
    "pd.set_option(\"display.max_columns\", dataset.shape[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b153149",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>year</th>\n",
       "      <th>loan_limit</th>\n",
       "      <th>Gender</th>\n",
       "      <th>approv_in_adv</th>\n",
       "      <th>loan_type</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>Credit_Worthiness</th>\n",
       "      <th>open_credit</th>\n",
       "      <th>business_or_commercial</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>rate_of_interest</th>\n",
       "      <th>Interest_rate_spread</th>\n",
       "      <th>Upfront_charges</th>\n",
       "      <th>term</th>\n",
       "      <th>Neg_ammortization</th>\n",
       "      <th>interest_only</th>\n",
       "      <th>lump_sum_payment</th>\n",
       "      <th>property_value</th>\n",
       "      <th>construction_type</th>\n",
       "      <th>occupancy_type</th>\n",
       "      <th>Secured_by</th>\n",
       "      <th>total_units</th>\n",
       "      <th>income</th>\n",
       "      <th>credit_type</th>\n",
       "      <th>Credit_Score</th>\n",
       "      <th>co-applicant_credit_type</th>\n",
       "      <th>age</th>\n",
       "      <th>submission_of_application</th>\n",
       "      <th>LTV</th>\n",
       "      <th>Region</th>\n",
       "      <th>Security_Type</th>\n",
       "      <th>Status</th>\n",
       "      <th>dtir1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24890</td>\n",
       "      <td>2019</td>\n",
       "      <td>cf</td>\n",
       "      <td>Sex Not Available</td>\n",
       "      <td>nopre</td>\n",
       "      <td>type1</td>\n",
       "      <td>p1</td>\n",
       "      <td>l1</td>\n",
       "      <td>nopc</td>\n",
       "      <td>nob/c</td>\n",
       "      <td>116500</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>360.0</td>\n",
       "      <td>not_neg</td>\n",
       "      <td>not_int</td>\n",
       "      <td>not_lpsm</td>\n",
       "      <td>118000.0</td>\n",
       "      <td>sb</td>\n",
       "      <td>pr</td>\n",
       "      <td>home</td>\n",
       "      <td>1U</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>EXP</td>\n",
       "      <td>758</td>\n",
       "      <td>CIB</td>\n",
       "      <td>25-34</td>\n",
       "      <td>to_inst</td>\n",
       "      <td>98.72881356</td>\n",
       "      <td>south</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24891</td>\n",
       "      <td>2019</td>\n",
       "      <td>cf</td>\n",
       "      <td>Male</td>\n",
       "      <td>nopre</td>\n",
       "      <td>type2</td>\n",
       "      <td>p1</td>\n",
       "      <td>l1</td>\n",
       "      <td>nopc</td>\n",
       "      <td>b/c</td>\n",
       "      <td>206500</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>360.0</td>\n",
       "      <td>not_neg</td>\n",
       "      <td>not_int</td>\n",
       "      <td>lpsm</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>sb</td>\n",
       "      <td>pr</td>\n",
       "      <td>home</td>\n",
       "      <td>1U</td>\n",
       "      <td>4980.0</td>\n",
       "      <td>EQUI</td>\n",
       "      <td>552</td>\n",
       "      <td>EXP</td>\n",
       "      <td>55-64</td>\n",
       "      <td>to_inst</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>North</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24892</td>\n",
       "      <td>2019</td>\n",
       "      <td>cf</td>\n",
       "      <td>Male</td>\n",
       "      <td>pre</td>\n",
       "      <td>type1</td>\n",
       "      <td>p1</td>\n",
       "      <td>l1</td>\n",
       "      <td>nopc</td>\n",
       "      <td>nob/c</td>\n",
       "      <td>406500</td>\n",
       "      <td>4.56</td>\n",
       "      <td>0.2</td>\n",
       "      <td>595.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>neg_amm</td>\n",
       "      <td>not_int</td>\n",
       "      <td>not_lpsm</td>\n",
       "      <td>508000.0</td>\n",
       "      <td>sb</td>\n",
       "      <td>pr</td>\n",
       "      <td>home</td>\n",
       "      <td>1U</td>\n",
       "      <td>9480.0</td>\n",
       "      <td>EXP</td>\n",
       "      <td>834</td>\n",
       "      <td>CIB</td>\n",
       "      <td>35-44</td>\n",
       "      <td>to_inst</td>\n",
       "      <td>80.01968504</td>\n",
       "      <td>south</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24893</td>\n",
       "      <td>2019</td>\n",
       "      <td>cf</td>\n",
       "      <td>Male</td>\n",
       "      <td>nopre</td>\n",
       "      <td>type1</td>\n",
       "      <td>p4</td>\n",
       "      <td>l1</td>\n",
       "      <td>nopc</td>\n",
       "      <td>nob/c</td>\n",
       "      <td>456500</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.681</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>360.0</td>\n",
       "      <td>not_neg</td>\n",
       "      <td>not_int</td>\n",
       "      <td>not_lpsm</td>\n",
       "      <td>658000.0</td>\n",
       "      <td>sb</td>\n",
       "      <td>pr</td>\n",
       "      <td>home</td>\n",
       "      <td>1U</td>\n",
       "      <td>11880.0</td>\n",
       "      <td>EXP</td>\n",
       "      <td>587</td>\n",
       "      <td>CIB</td>\n",
       "      <td>45-54</td>\n",
       "      <td>not_inst</td>\n",
       "      <td>69.3768997</td>\n",
       "      <td>North</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24894</td>\n",
       "      <td>2019</td>\n",
       "      <td>cf</td>\n",
       "      <td>Joint</td>\n",
       "      <td>pre</td>\n",
       "      <td>type1</td>\n",
       "      <td>p1</td>\n",
       "      <td>l1</td>\n",
       "      <td>nopc</td>\n",
       "      <td>nob/c</td>\n",
       "      <td>696500</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.3042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>not_neg</td>\n",
       "      <td>not_int</td>\n",
       "      <td>not_lpsm</td>\n",
       "      <td>758000.0</td>\n",
       "      <td>sb</td>\n",
       "      <td>pr</td>\n",
       "      <td>home</td>\n",
       "      <td>1U</td>\n",
       "      <td>10440.0</td>\n",
       "      <td>CRIF</td>\n",
       "      <td>602</td>\n",
       "      <td>EXP</td>\n",
       "      <td>25-34</td>\n",
       "      <td>not_inst</td>\n",
       "      <td>91.88654354</td>\n",
       "      <td>North</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  year loan_limit             Gender approv_in_adv loan_type  \\\n",
       "0  24890  2019         cf  Sex Not Available         nopre     type1   \n",
       "1  24891  2019         cf               Male         nopre     type2   \n",
       "2  24892  2019         cf               Male           pre     type1   \n",
       "3  24893  2019         cf               Male         nopre     type1   \n",
       "4  24894  2019         cf              Joint           pre     type1   \n",
       "\n",
       "  loan_purpose Credit_Worthiness open_credit business_or_commercial  \\\n",
       "0           p1                l1        nopc                  nob/c   \n",
       "1           p1                l1        nopc                    b/c   \n",
       "2           p1                l1        nopc                  nob/c   \n",
       "3           p4                l1        nopc                  nob/c   \n",
       "4           p1                l1        nopc                  nob/c   \n",
       "\n",
       "   loan_amount rate_of_interest Interest_rate_spread Upfront_charges   term  \\\n",
       "0       116500             <NA>                 <NA>            <NA>  360.0   \n",
       "1       206500             <NA>                 <NA>            <NA>  360.0   \n",
       "2       406500             4.56                  0.2           595.0  360.0   \n",
       "3       456500             4.25                0.681            <NA>  360.0   \n",
       "4       696500              4.0               0.3042             0.0  360.0   \n",
       "\n",
       "  Neg_ammortization interest_only lump_sum_payment property_value  \\\n",
       "0           not_neg       not_int         not_lpsm       118000.0   \n",
       "1           not_neg       not_int             lpsm           <NA>   \n",
       "2           neg_amm       not_int         not_lpsm       508000.0   \n",
       "3           not_neg       not_int         not_lpsm       658000.0   \n",
       "4           not_neg       not_int         not_lpsm       758000.0   \n",
       "\n",
       "  construction_type occupancy_type Secured_by total_units   income  \\\n",
       "0                sb             pr       home          1U   1740.0   \n",
       "1                sb             pr       home          1U   4980.0   \n",
       "2                sb             pr       home          1U   9480.0   \n",
       "3                sb             pr       home          1U  11880.0   \n",
       "4                sb             pr       home          1U  10440.0   \n",
       "\n",
       "  credit_type  Credit_Score co-applicant_credit_type    age  \\\n",
       "0         EXP           758                      CIB  25-34   \n",
       "1        EQUI           552                      EXP  55-64   \n",
       "2         EXP           834                      CIB  35-44   \n",
       "3         EXP           587                      CIB  45-54   \n",
       "4        CRIF           602                      EXP  25-34   \n",
       "\n",
       "  submission_of_application          LTV Region Security_Type  Status dtir1  \n",
       "0                   to_inst  98.72881356  south        direct       1  45.0  \n",
       "1                   to_inst         <NA>  North        direct       1  <NA>  \n",
       "2                   to_inst  80.01968504  south        direct       0  46.0  \n",
       "3                  not_inst   69.3768997  North        direct       0  42.0  \n",
       "4                  not_inst  91.88654354  North        direct       0  39.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb663d8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We have no use of the column ID\n",
    "dataset.drop(['ID'], axis='columns', inplace=True)\n",
    "# dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82c4a6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2019\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will check how many years of data this file has using year column\n",
    "\n",
    "dataset.loc[:,'year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bf55b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file contains only year 2019 data. \n",
    "# This makes the year column unnecessary. So, we will drop this column instead\n",
    "\n",
    "dataset.drop('year', axis='columns', inplace=True)\n",
    "# dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62d67127",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: 32\n",
      "rows: 148670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We have drop 2 columns namely ID and year from our dataset. Now we have following number of features in our data\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "n_rows, n_features = dataset.shape\n",
    "print (f\"features: {n_features}\")\n",
    "print (f\"rows: {n_rows}\")\n",
    "\n",
    "# We also need to make sure that we have enough data to test all the categories\n",
    "# Status is our dependent feature\n",
    "dist = dataset.groupby(by=\"Status\").size()\n",
    "\n",
    "sns.barplot(x=dist.index.values_host, y=dist.values.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e790cbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_limit                    3344\n",
       "Gender                           0\n",
       "approv_in_adv                  908\n",
       "loan_type                        0\n",
       "loan_purpose                   134\n",
       "Credit_Worthiness                0\n",
       "open_credit                      0\n",
       "business_or_commercial           0\n",
       "loan_amount                      0\n",
       "rate_of_interest             36439\n",
       "Interest_rate_spread         36639\n",
       "Upfront_charges              39642\n",
       "term                            41\n",
       "Neg_ammortization              121\n",
       "interest_only                    0\n",
       "lump_sum_payment                 0\n",
       "property_value               15098\n",
       "construction_type                0\n",
       "occupancy_type                   0\n",
       "Secured_by                       0\n",
       "total_units                      0\n",
       "income                        9150\n",
       "credit_type                      0\n",
       "Credit_Score                     0\n",
       "co-applicant_credit_type         0\n",
       "age                            200\n",
       "submission_of_application      200\n",
       "LTV                          15098\n",
       "Region                           0\n",
       "Security_Type                    0\n",
       "Status                           0\n",
       "dtir1                        24121\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking how many columns has null values in them\n",
    "\n",
    "null_cols = dataset.isnull().sum() # we can also use isnull method both are same\n",
    "null_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da3b5f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loan_limit',\n",
       " 'approv_in_adv',\n",
       " 'loan_purpose',\n",
       " 'rate_of_interest',\n",
       " 'Interest_rate_spread',\n",
       " 'Upfront_charges',\n",
       " 'term',\n",
       " 'Neg_ammortization',\n",
       " 'property_value',\n",
       " 'income',\n",
       " 'age',\n",
       " 'submission_of_application',\n",
       " 'LTV',\n",
       " 'dtir1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting column names which has atleast 1 null value\n",
    "\n",
    "cols_with_na_values = null_cols.index[(null_cols > 0)].values_host.tolist()\n",
    "cols_with_na_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa94f81b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EXPECTED:\n",
    "# As the datatypes of some values are in object we cannot perform mathematical analysis on them\n",
    "\n",
    "# dataset[['loan_limit']].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66eaf923",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'cudf.core.dataframe.DataFrame'>\n",
      "RangeIndex: 148670 entries, 0 to 148669\n",
      "Data columns (total 32 columns):\n",
      " #   Column                     Non-Null Count   Dtype\n",
      "---  ------                     --------------   -----\n",
      " 0   loan_limit                 145326 non-null  object\n",
      " 1   Gender                     148670 non-null  object\n",
      " 2   approv_in_adv              147762 non-null  object\n",
      " 3   loan_type                  148670 non-null  object\n",
      " 4   loan_purpose               148536 non-null  object\n",
      " 5   Credit_Worthiness          148670 non-null  object\n",
      " 6   open_credit                148670 non-null  object\n",
      " 7   business_or_commercial     148670 non-null  object\n",
      " 8   loan_amount                148670 non-null  int64\n",
      " 9   rate_of_interest           112231 non-null  float64\n",
      " 10  Interest_rate_spread       112031 non-null  float64\n",
      " 11  Upfront_charges            109028 non-null  float64\n",
      " 12  term                       148629 non-null  float64\n",
      " 13  Neg_ammortization          148549 non-null  object\n",
      " 14  interest_only              148670 non-null  object\n",
      " 15  lump_sum_payment           148670 non-null  object\n",
      " 16  property_value             133572 non-null  float64\n",
      " 17  construction_type          148670 non-null  object\n",
      " 18  occupancy_type             148670 non-null  object\n",
      " 19  Secured_by                 148670 non-null  object\n",
      " 20  total_units                148670 non-null  object\n",
      " 21  income                     139520 non-null  float64\n",
      " 22  credit_type                148670 non-null  object\n",
      " 23  Credit_Score               148670 non-null  int64\n",
      " 24  co-applicant_credit_type   148670 non-null  object\n",
      " 25  age                        148470 non-null  object\n",
      " 26  submission_of_application  148470 non-null  object\n",
      " 27  LTV                        133572 non-null  float64\n",
      " 28  Region                     148670 non-null  object\n",
      " 29  Security_Type              148670 non-null  object\n",
      " 30  Status                     148670 non-null  int64\n",
      " 31  dtir1                      124549 non-null  float64\n",
      "dtypes: float64(8), int64(3), object(21)\n",
      "memory usage: 38.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# checking the datatype of different columns\n",
    "# As we can see loan_limit is object\n",
    "\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "351e8cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    <NA>\n",
      "1      cf\n",
      "2     ncf\n",
      "Name: loan_limit, dtype: object\n",
      "0               Female\n",
      "1                Joint\n",
      "2                 Male\n",
      "3    Sex Not Available\n",
      "Name: Gender, dtype: object\n",
      "0     <NA>\n",
      "1    nopre\n",
      "2      pre\n",
      "Name: approv_in_adv, dtype: object\n",
      "0    type1\n",
      "1    type2\n",
      "2    type3\n",
      "Name: loan_type, dtype: object\n",
      "0    <NA>\n",
      "1      p1\n",
      "2      p2\n",
      "3      p3\n",
      "4      p4\n",
      "Name: loan_purpose, dtype: object\n",
      "0    l1\n",
      "1    l2\n",
      "Name: Credit_Worthiness, dtype: object\n",
      "0    nopc\n",
      "1     opc\n",
      "Name: open_credit, dtype: object\n",
      "0      b/c\n",
      "1    nob/c\n",
      "Name: business_or_commercial, dtype: object\n",
      "0        16500\n",
      "1        26500\n",
      "2        36500\n",
      "3        46500\n",
      "4        56500\n",
      "        ...   \n",
      "206    2926500\n",
      "207    2986500\n",
      "208    3006500\n",
      "209    3346500\n",
      "210    3576500\n",
      "Name: loan_amount, Length: 211, dtype: int64\n",
      "0       <NA>\n",
      "1        0.0\n",
      "2      2.125\n",
      "3       2.25\n",
      "4      2.375\n",
      "       ...  \n",
      "127    7.125\n",
      "128    7.375\n",
      "129      7.5\n",
      "130     7.75\n",
      "131      8.0\n",
      "Name: rate_of_interest, Length: 132, dtype: float64\n",
      "0           <NA>\n",
      "1         -3.638\n",
      "2        -1.0841\n",
      "3         -1.047\n",
      "4        -1.0462\n",
      "          ...   \n",
      "22512     2.5932\n",
      "22513     2.6368\n",
      "22514     2.7227\n",
      "22515     2.8854\n",
      "22516      3.357\n",
      "Name: Interest_rate_spread, Length: 22517, dtype: float64\n",
      "0            <NA>\n",
      "1             0.0\n",
      "2            0.03\n",
      "3            0.06\n",
      "4            0.35\n",
      "           ...   \n",
      "58267    37604.38\n",
      "58268     38375.0\n",
      "58269     38437.5\n",
      "58270    53485.78\n",
      "58271     60000.0\n",
      "Name: Upfront_charges, Length: 58272, dtype: float64\n",
      "0      <NA>\n",
      "1      96.0\n",
      "2     108.0\n",
      "3     120.0\n",
      "4     132.0\n",
      "5     144.0\n",
      "6     156.0\n",
      "7     165.0\n",
      "8     168.0\n",
      "9     180.0\n",
      "10    192.0\n",
      "11    204.0\n",
      "12    216.0\n",
      "13    228.0\n",
      "14    240.0\n",
      "15    252.0\n",
      "16    264.0\n",
      "17    276.0\n",
      "18    280.0\n",
      "19    288.0\n",
      "20    300.0\n",
      "21    312.0\n",
      "22    322.0\n",
      "23    324.0\n",
      "24    336.0\n",
      "25    348.0\n",
      "26    360.0\n",
      "Name: term, dtype: float64\n",
      "0       <NA>\n",
      "1    neg_amm\n",
      "2    not_neg\n",
      "Name: Neg_ammortization, dtype: object\n",
      "0    int_only\n",
      "1     not_int\n",
      "Name: interest_only, dtype: object\n",
      "0        lpsm\n",
      "1    not_lpsm\n",
      "Name: lump_sum_payment, dtype: object\n",
      "0            <NA>\n",
      "1          8000.0\n",
      "2         18000.0\n",
      "3         28000.0\n",
      "4         38000.0\n",
      "          ...    \n",
      "381     9268000.0\n",
      "382    10008000.0\n",
      "383    11008000.0\n",
      "384    12008000.0\n",
      "385    16508000.0\n",
      "Name: property_value, Length: 386, dtype: float64\n",
      "0    mh\n",
      "1    sb\n",
      "Name: construction_type, dtype: object\n",
      "0    ir\n",
      "1    pr\n",
      "2    sr\n",
      "Name: occupancy_type, dtype: object\n",
      "0    home\n",
      "1    land\n",
      "Name: Secured_by, dtype: object\n",
      "0    1U\n",
      "1    2U\n",
      "2    3U\n",
      "3    4U\n",
      "Name: total_units, dtype: object\n",
      "0           <NA>\n",
      "1            0.0\n",
      "2           60.0\n",
      "3          120.0\n",
      "4          180.0\n",
      "          ...   \n",
      "997     329460.0\n",
      "998     335880.0\n",
      "999     374400.0\n",
      "1000    377220.0\n",
      "1001    578580.0\n",
      "Name: income, Length: 1002, dtype: float64\n",
      "0     CIB\n",
      "1    CRIF\n",
      "2    EQUI\n",
      "3     EXP\n",
      "Name: credit_type, dtype: object\n",
      "0      500\n",
      "1      501\n",
      "2      502\n",
      "3      503\n",
      "4      504\n",
      "      ... \n",
      "396    896\n",
      "397    897\n",
      "398    898\n",
      "399    899\n",
      "400    900\n",
      "Name: Credit_Score, Length: 401, dtype: int64\n",
      "0    CIB\n",
      "1    EXP\n",
      "Name: co-applicant_credit_type, dtype: object\n",
      "0     <NA>\n",
      "1    25-34\n",
      "2    35-44\n",
      "3    45-54\n",
      "4    55-64\n",
      "5    65-74\n",
      "6      <25\n",
      "7      >74\n",
      "Name: age, dtype: object\n",
      "0        <NA>\n",
      "1    not_inst\n",
      "2     to_inst\n",
      "Name: submission_of_application, dtype: object\n",
      "0              <NA>\n",
      "1       0.967478198\n",
      "2       2.072942643\n",
      "3       2.767587397\n",
      "4        2.81374502\n",
      "           ...     \n",
      "8480        2956.25\n",
      "8481        4706.25\n",
      "8482        5206.25\n",
      "8483        6706.25\n",
      "8484        7831.25\n",
      "Name: LTV, Length: 8485, dtype: float64\n",
      "0         North\n",
      "1    North-East\n",
      "2       central\n",
      "3         south\n",
      "Name: Region, dtype: object\n",
      "0    Indriect\n",
      "1      direct\n",
      "Name: Security_Type, dtype: object\n",
      "0    0\n",
      "1    1\n",
      "Name: Status, dtype: int64\n",
      "0     <NA>\n",
      "1      5.0\n",
      "2      6.0\n",
      "3      7.0\n",
      "4      8.0\n",
      "5      9.0\n",
      "6     10.0\n",
      "7     11.0\n",
      "8     12.0\n",
      "9     13.0\n",
      "10    14.0\n",
      "11    15.0\n",
      "12    16.0\n",
      "13    17.0\n",
      "14    18.0\n",
      "15    19.0\n",
      "16    20.0\n",
      "17    21.0\n",
      "18    22.0\n",
      "19    23.0\n",
      "20    24.0\n",
      "21    25.0\n",
      "22    26.0\n",
      "23    27.0\n",
      "24    28.0\n",
      "25    29.0\n",
      "26    30.0\n",
      "27    31.0\n",
      "28    32.0\n",
      "29    33.0\n",
      "30    34.0\n",
      "31    35.0\n",
      "32    36.0\n",
      "33    37.0\n",
      "34    38.0\n",
      "35    39.0\n",
      "36    40.0\n",
      "37    41.0\n",
      "38    42.0\n",
      "39    43.0\n",
      "40    44.0\n",
      "41    45.0\n",
      "42    46.0\n",
      "43    47.0\n",
      "44    48.0\n",
      "45    49.0\n",
      "46    50.0\n",
      "47    51.0\n",
      "48    52.0\n",
      "49    53.0\n",
      "50    54.0\n",
      "51    55.0\n",
      "52    56.0\n",
      "53    57.0\n",
      "54    58.0\n",
      "55    59.0\n",
      "56    60.0\n",
      "57    61.0\n",
      "Name: dtir1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Sometimes variables have float or int values but they are stored as object in that case we can change the \n",
    "for column in dataset.columns:\n",
    "    print (dataset.loc[:, column].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4846590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. loan_limit has only 3 which are [[None, 'cf', 'ncf']] unique values - CATEGORICAL\n",
      "2. Gender has only 4 which are [['Female', 'Joint', 'Male', 'Sex Not Available']] unique values - CATEGORICAL\n",
      "3. approv_in_adv has only 3 which are [[None, 'nopre', 'pre']] unique values - CATEGORICAL\n",
      "4. loan_type has only 3 which are [['type1', 'type2', 'type3']] unique values - CATEGORICAL\n",
      "5. loan_purpose has only 5 which are [[None, 'p1', 'p2', 'p3', 'p4']] unique values - CATEGORICAL\n",
      "6. Credit_Worthiness has only 2 which are [['l1', 'l2']] unique values - CATEGORICAL\n",
      "7. open_credit has only 2 which are [['nopc', 'opc']] unique values - CATEGORICAL\n",
      "8. business_or_commercial has only 2 which are [['b/c', 'nob/c']] unique values - CATEGORICAL\n",
      "14. Neg_ammortization has only 3 which are [[None, 'neg_amm', 'not_neg']] unique values - CATEGORICAL\n",
      "15. interest_only has only 2 which are [['int_only', 'not_int']] unique values - CATEGORICAL\n",
      "16. lump_sum_payment has only 2 which are [['lpsm', 'not_lpsm']] unique values - CATEGORICAL\n",
      "18. construction_type has only 2 which are [['mh', 'sb']] unique values - CATEGORICAL\n",
      "19. occupancy_type has only 3 which are [['ir', 'pr', 'sr']] unique values - CATEGORICAL\n",
      "20. Secured_by has only 2 which are [['home', 'land']] unique values - CATEGORICAL\n",
      "21. total_units has only 4 which are [['1U', '2U', '3U', '4U']] unique values - CATEGORICAL\n",
      "23. credit_type has only 4 which are [['CIB', 'CRIF', 'EQUI', 'EXP']] unique values - CATEGORICAL\n",
      "25. co-applicant_credit_type has only 2 which are [['CIB', 'EXP']] unique values - CATEGORICAL\n",
      "26. age has only 8 which are [[None, '25-34', '35-44', '45-54', '55-64', '65-74', '<25', '>74']] unique values - CATEGORICAL\n",
      "27. submission_of_application has only 3 which are [[None, 'not_inst', 'to_inst']] unique values - CATEGORICAL\n",
      "29. Region has only 4 which are [['North', 'North-East', 'central', 'south']] unique values - CATEGORICAL\n",
      "30. Security_Type has only 2 which are [['Indriect', 'direct']] unique values - CATEGORICAL\n"
     ]
    }
   ],
   "source": [
    "# As you can see loan_limit variable is a categoriacal variable and has 2 different categories cf and ncf\n",
    "\n",
    "# We can try to find columns which has categorical values with based of what number of unique values they have. \n",
    "\n",
    "# Here we will set it to 10. So, if any columns has < 10 unique values in them we will categorize it as a\\\n",
    "# categorical column\n",
    "    \n",
    "n_unique_values = 10\n",
    "all_columns = dataset.columns.to_list()\n",
    "categorical_columns = []\n",
    "\n",
    "for i, column in enumerate(all_columns):\n",
    "    dtype = dataset[column].dtype\n",
    "    temp = dataset.loc[:, column].unique().to_arrow().to_pylist()\n",
    "    if len(temp) < n_unique_values and dtype == 'object':\n",
    "        print (f\"{i+1}. {column} has only {len(temp)} which are [{temp}] unique values - CATEGORICAL\") \n",
    "        categorical_columns.append(column)\n",
    "    del temp, dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0ce30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender feature has value 'Sex Not Available', which is kind of NaN value.\n",
    "\n",
    "# We will replace those values with np.nan value.\n",
    "\n",
    "dataset[\"Gender\"] = dataset[\"Gender\"].replace(\"Sex Not Available\", np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9477108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loan_limit', 'approv_in_adv', 'loan_purpose', 'rate_of_interest', 'Interest_rate_spread', 'Upfront_charges', 'term', 'Neg_ammortization', 'property_value', 'income', 'age', 'submission_of_application', 'LTV', 'dtir1', 'Gender']\n"
     ]
    }
   ],
   "source": [
    "# Since now Gender feature has NaN values we need to append this feature in our cols_with_na_values list if it \\\n",
    "# did not exists in it already\n",
    "\n",
    "if \"Gender\" not in cols_with_na_values:\n",
    "    cols_with_na_values.append(\"Gender\")\n",
    "    print (cols_with_na_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6dfa3416",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_limit                    3344\n",
       "Gender                       37659\n",
       "approv_in_adv                  908\n",
       "loan_type                        0\n",
       "loan_purpose                   134\n",
       "Credit_Worthiness                0\n",
       "open_credit                      0\n",
       "business_or_commercial           0\n",
       "loan_amount                      0\n",
       "rate_of_interest             36439\n",
       "Interest_rate_spread         36639\n",
       "Upfront_charges              39642\n",
       "term                            41\n",
       "Neg_ammortization              121\n",
       "interest_only                    0\n",
       "lump_sum_payment                 0\n",
       "property_value               15098\n",
       "construction_type                0\n",
       "occupancy_type                   0\n",
       "Secured_by                       0\n",
       "total_units                      0\n",
       "income                        9150\n",
       "credit_type                      0\n",
       "Credit_Score                     0\n",
       "co-applicant_credit_type         0\n",
       "age                            200\n",
       "submission_of_application      200\n",
       "LTV                          15098\n",
       "Region                           0\n",
       "Security_Type                    0\n",
       "Status                           0\n",
       "dtir1                        24121\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dad34f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_limit</th>\n",
       "      <th>approv_in_adv</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>rate_of_interest</th>\n",
       "      <th>Interest_rate_spread</th>\n",
       "      <th>Upfront_charges</th>\n",
       "      <th>term</th>\n",
       "      <th>Neg_ammortization</th>\n",
       "      <th>property_value</th>\n",
       "      <th>income</th>\n",
       "      <th>age</th>\n",
       "      <th>submission_of_application</th>\n",
       "      <th>LTV</th>\n",
       "      <th>dtir1</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1</td>\n",
       "      <td>118000.0</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>98.72881356</td>\n",
       "      <td>45.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>4980.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.56</td>\n",
       "      <td>0.2</td>\n",
       "      <td>595.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0</td>\n",
       "      <td>508000.0</td>\n",
       "      <td>9480.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>80.01968504</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.681</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1</td>\n",
       "      <td>658000.0</td>\n",
       "      <td>11880.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>69.3768997</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.3042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1</td>\n",
       "      <td>758000.0</td>\n",
       "      <td>10440.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91.88654354</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  loan_limit approv_in_adv loan_purpose rate_of_interest Interest_rate_spread  \\\n",
       "0          0             0            0             <NA>                 <NA>   \n",
       "1          0             0            0             <NA>                 <NA>   \n",
       "2          0             1            0             4.56                  0.2   \n",
       "3          0             0            3             4.25                0.681   \n",
       "4          0             1            0              4.0               0.3042   \n",
       "\n",
       "  Upfront_charges   term Neg_ammortization property_value   income age  \\\n",
       "0            <NA>  360.0                 1       118000.0   1740.0   0   \n",
       "1            <NA>  360.0                 1           <NA>   4980.0   3   \n",
       "2           595.0  360.0                 0       508000.0   9480.0   1   \n",
       "3            <NA>  360.0                 1       658000.0  11880.0   2   \n",
       "4             0.0  360.0                 1       758000.0  10440.0   0   \n",
       "\n",
       "  submission_of_application          LTV dtir1 Gender  \n",
       "0                         1  98.72881356  45.0   <NA>  \n",
       "1                         1         <NA>  <NA>      2  \n",
       "2                         1  80.01968504  46.0      2  \n",
       "3                         0   69.3768997  42.0      2  \n",
       "4                         0  91.88654354  39.0      1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before we can perform any kind of statistical analysis we need to convert the dataset features in right format.\n",
    "\n",
    "# We need to encode all the categorical features but also need to make sure that NaN values do not get encoded \\\n",
    "# in the process.\n",
    "\n",
    "from cuml.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder(verbose=True)\n",
    "for column in categorical_columns:\n",
    "    temp = dataset.loc[ dataset[column].notnull(), column ] \n",
    "    dataset.loc[ dataset[column].notnull(), [column,] ] = le.fit_transform(temp)\n",
    "    del temp\n",
    "del le\n",
    "dataset[cols_with_na_values].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c86ea41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. loan_amount needs to be scaled, it has mean value of 331117.74 and standard deviation value of 183909.31\n",
      "2. Upfront_charges needs to be scaled, it has mean value of 3225.00 and standard deviation value of 3251.12\n",
      "3. term needs to be scaled, it has mean value of 335.14 and standard deviation value of 58.41\n",
      "4. property_value needs to be scaled, it has mean value of 497893.47 and standard deviation value of 359935.32\n",
      "5. income needs to be scaled, it has mean value of 6957.34 and standard deviation value of 6496.59\n",
      "6. Credit_Score needs to be scaled, it has mean value of 699.79 and standard deviation value of 115.88\n",
      "7. LTV needs to be scaled, it has mean value of 72.75 and standard deviation value of 39.97\n",
      "8. dtir1 needs to be scaled, it has mean value of 37.73 and standard deviation value of 10.55\n",
      "\n",
      "These columns needs to be scaled ['loan_amount', 'Upfront_charges', 'term', 'property_value', 'income', 'Credit_Score', 'LTV', 'dtir1']\n"
     ]
    }
   ],
   "source": [
    "# As we see in the dataset some are continuous features, those usually don't follow normal/gaussian distribution \\\n",
    "# and making it necessary to perform standard normal distribution using mean and std.\n",
    "\n",
    "# normal/gaussian distribution: mean = 0 and std = 1\n",
    "\n",
    "# These values needs to be scaled properly for better accuracy.\n",
    "\n",
    "# If the difference between min and max is >= 11 then, we will count it as continuous feature and perform \\\n",
    "# standard scaling.\n",
    "\n",
    "min_ge_value = 10\n",
    "\n",
    "scale_columns = []\n",
    "for column in dataset.columns:\n",
    "    min_ = int(dataset[column].min())\n",
    "    max_ = int(dataset[column].max())\n",
    "    if abs(max_ - min_) >= min_ge_value:\n",
    "        mean = dataset[column].mean()\n",
    "        std = dataset[column].std()\n",
    "        print (f\"{len(scale_columns)+1}. {column} needs to be scaled, it has mean value of {mean:.2f} and standard deviation value of {std:.2f}\")\n",
    "        scale_columns.append(column)\n",
    "print ()\n",
    "print (f\"These columns needs to be scaled {scale_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916a713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# As we see in the dataset some column values varies from a very low to very high value.\n",
    "\n",
    "# These values needs to be scaled properly for better accuracy.\n",
    "\n",
    "# We can use StandardScaler class to do the same.\n",
    "\n",
    "# This also can be said as this data does not follow normal / gaussian distribution (mean != 0 and std != 1)\n",
    "# This means that we need to perform standard normal distribution using mean and std\n",
    "\n",
    "from cuml.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "if len(scale_columns) > 1:\n",
    "    dataset[scale_columns] = scaler.fit_transform(dataset[scale_columns])\n",
    "elif len(scale_columns) == 1:\n",
    "    dataset[column] = scaler.fit_transform(dataset[column].values.reshape(-1,1))\n",
    "\n",
    "del scaler\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857039b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b947e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we need to remove NaN values, but in this dataset we have a lot of NaN values if we just remove those, \\\n",
    "# there is a chance that we may end up losing lot of values or may be an entire category (tested).\n",
    "\n",
    "# We will create small models depending on the feature type we are working on continuous / categorical what \\\n",
    "# algorithms we will be using.\n",
    "\n",
    "# For each feature with NaN values we need to find another feature which has the best negative / positive relation\n",
    "# with it and based on that feature we will train our model and find the approximate values.\n",
    "\n",
    "# We will do this using the heatmap.\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1e6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_na_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e0449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Before creating the heatmap, in this dataset lots of column have object datatype which are actually carrying \\\n",
    "# int after using label encoder.\n",
    "\n",
    "# We need to convert features to correct datatypes for statistical analysis.\n",
    "\n",
    "for column in dataset.columns:\n",
    "    i = 0\n",
    "    column = str(column)\n",
    "    if dataset[column].dtype == 'object':\n",
    "        while True:\n",
    "            temp = dataset.loc[i, column]\n",
    "            i += 1\n",
    "            if type(temp) != pd._libs.missing.NAType:\n",
    "                isFloat = False\n",
    "                try:\n",
    "                    float(temp)\n",
    "                    isFloat = True\n",
    "                except:\n",
    "                    isFloat = False\n",
    "                if temp.isnumeric():\n",
    "                    print (f\"Coverting feature {column} type from {dataset[column].dtype} to int\")\n",
    "                    dataset[column] = dataset[column].astype(int, copy=False)\n",
    "                elif isFloat:\n",
    "                    print (f\"Coverting feature {column} type from {dataset[column].dtype} to float\")\n",
    "                    dataset[column] = dataset[column].astype(float, copy=False)\n",
    "                else:\n",
    "                    print (f\"Leaving {column} as it is {dataset[column].dtype}\")\n",
    "                break\n",
    "\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb5474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f60364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_map = dataset.to_pandas().corr()\n",
    "corr_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21849ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 20), dpi=100) \n",
    "sns.heatmap(corr_map, annot=True, annot_kws={\"fontsize\":12}, linecolor='white', \\\n",
    "            linewidth=1, fmt='.3f', cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3b0264",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Before removing the features or rows lets check the relationship of all the columns with our dependent variable\n",
    "\n",
    "corr_label = np.array([\"high\", \"moderate\", \"low\", \"no\"])\n",
    "pos_neg_corr_label = np.array([\"positive\", \"negative\"])\n",
    "\n",
    "correlations = {}\n",
    "for column in cols_with_na_values:\n",
    "    dict_ = corr_map[column].drop(labels=column).to_dict()\n",
    "    values = list(dict_.values())\n",
    "    keys = list(dict_.keys())\n",
    "    value = max( [abs(min(values)), abs(max(values))] )\n",
    "    relation = ([\n",
    "        value > 0.7 and value <= 1.0, \n",
    "        value > 0.5 and value <= 0.7,\n",
    "        value > 0.0 and value <= 0.5, \n",
    "        value == 0.0,\n",
    "    ]) \n",
    "    try:\n",
    "        key = keys[values.index(value)]\n",
    "    except ValueError:\n",
    "        value *= -1\n",
    "        key = keys[values.index(value)]    \n",
    "    cor_label = corr_label[relation][0]\n",
    "    if cor_label in correlations.keys():\n",
    "        temp = correlations.get(cor_label)\n",
    "        temp[column] = [value, key]\n",
    "        correlations[cor_label].update(temp)\n",
    "    else:\n",
    "        correlations[cor_label] = {column: [value, key]}\n",
    "    print (f\"{column} has {corr_label[relation][0]} {pos_neg_corr_label[[value >= 0, value < 0]][0]} \" +\\\n",
    "    f\"correlation with {key} as {value:.2f}\")\n",
    "\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0163b53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keys of the correlation dictionary is out dependent variable (y), we need to check that the data which is not \\\n",
    "# present in dependent variable (y) is present in independent variable (X)\n",
    "\n",
    "# Here we can see almost the same amount of values we have as null in both features and we can't use any feature \\\n",
    "# to build model and predict missing values\n",
    "\n",
    "for cor in correlations.keys():\n",
    "    print ('\\n', '-'*10, 'Correlation:', cor.upper(), '-'*10, '\\n')\n",
    "    for sub in correlations.get(cor):\n",
    "        temp = correlations.get(cor).get(sub)\n",
    "        temp_data = dataset[[sub,temp[1]]]\n",
    "        final = temp_data[ temp_data[sub].isna() ][temp[1]].isna().sum()\n",
    "        print (f\"{sub.upper()} vs. {temp[1].upper()}: Corr: {temp[0]:.2f}\")\n",
    "        print (f\"Out of {temp_data[sub].isna().sum()} nan values in feature {sub.upper()}, \" +\\\n",
    "               f\"{temp_data[sub].isna().sum()-final} in {temp[1].upper()} are present and {final} are absent\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72c5b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the data with low correlation, we cannot built a model for that\n",
    "\n",
    "# We have to have moderate to high correlation to make accurate predictions for the missing data using \\\n",
    "# correlated features\n",
    "\n",
    "# But for the moderate correlation we can see that almost all nan data are also not present in independent feature\n",
    "\n",
    "# We have only one column for which we can build model, i.e. PROPERTY_VALUE vs. LOAN_AMOUNT: Corr: 0.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959a462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_vals = dataset[\"Status\"].unique().values.tolist()\n",
    "\n",
    "for status in status_vals:\n",
    "    if status is np.nan:\n",
    "        print (\"Status value is NaN\")\n",
    "        continue\n",
    "    total_samples = dataset[dataset[\"Status\"] == status].shape[0]\n",
    "    max_null_vals = np.max(dataset[dataset[\"Status\"] == status].isnull().sum().values)\n",
    "    msg = f\"Total samples with status as {status} are {total_samples} out of those {max_null_vals} \" +\\\n",
    "    \"samples have null values\"\n",
    "    print (msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d1b73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replacing null values in categorical features with mode of the feature\n",
    "\n",
    "for col in categorical_columns:\n",
    "    for status in status_vals:\n",
    "        mode = dataset[dataset[\"Status\"] == status][col].mode()[0]\n",
    "        if col in categorical_columns:\n",
    "            msg = f\"Replacing categorical feature \\\"{col}\\\" in status: {status} with Mode: {mode}\"\n",
    "        else:\n",
    "            msg = f\"Replacing continous feature \\\"{col}\\\" in status: {status} with Mode: {mode}\"\n",
    "        print (msg)\n",
    "        dataset[dataset[\"Status\"] == status] = dataset[dataset[\"Status\"] == status].fillna(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474be7e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1079298",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b2b6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cuml.linear_model import LinearRegression\n",
    "from cuml.model_selection import train_test_split\n",
    "# from cuml.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d34cf76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = dataset[\"Status\"]\n",
    "dataset.drop([\"Status\", ], axis=1, inplace=True)\n",
    "X = dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=99, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91675b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14364cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf_reg = rf.fit(X_train, y_train)\n",
    "\n",
    "print (f\"Train score: {rf_reg.score(X_train, y_train)}\")\n",
    "print (f\"Test score: {rf_reg.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.metrics import confusion_matrix\n",
    "\n",
    "y_pred = rf_reg.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92545ae6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay(cm.get(), display_labels=rf_reg.classes_.to_numpy()).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ff65be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test[[y_test == 0,]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc21776",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# params = {\n",
    "#     'alpha': [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0],\n",
    "#     'solver': ['auto', 'svd', 'sag', 'saga', 'lbfgs'],\n",
    "#     'max_iter': [x for x in np.arange(1000, 20000, step=1000)],\n",
    "#     'tol': [1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n",
    "# }\n",
    "\n",
    "# ridge = Ridge()\n",
    "# clf = GridSearchCV(ridge, params, n_jobs=-1)\n",
    "# clf.fit(X_train.get().reshape(-1,1), y_train.get().reshape(-1,1))\n",
    "\n",
    "# print ('best solver:', clf.best_estimator_)\n",
    "# print ('best params:', clf.best_params_)\n",
    "# print ('best score:', clf.best_score_)\n",
    "# print ('scoring: ', clf.scoring)\n",
    "# print ('estimator:', clf.estimator)\n",
    "# print ('score:', clf.score(X_train.get().reshape(-1,1), y_train.get().reshape(-1,1)))\n",
    "\n",
    "# # reg = ridge.fit(X_train.get().reshape(-1,1), y_train.get().reshape(-1,1))\n",
    "\n",
    "# # reg.score(X_train.get().reshape(-1,1), y_train.get().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f5a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = clf.predict(X_test.get().reshape(-1,1)) #  y_test.get().reshape(-1,1)\n",
    "# # y_pred\n",
    "\n",
    "# clf.score(X_test.get().reshape(-1,1), y_test.get().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9cf473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving sklearn model\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "filename = './finalized_model.sav'\n",
    "if os.path.exists(filename):\n",
    "    os.remove(filename)\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263faa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import Perceptron\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# import warnings\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# params = {\n",
    "#     'penalty': ['l1', 'l2',],\n",
    "#     'alpha': [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0],\n",
    "#     'max_iter': [x for x in np.arange(50, 2001, 50)],\n",
    "#     'tol': [1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n",
    "#     'n_jobs': [-1,],\n",
    "#     'random_state': [99,],\n",
    "#     'early_stopping': [True, ],\n",
    "# }\n",
    "\n",
    "# p = Perceptron()\n",
    "# clf2 = GridSearchCV(p, params)\n",
    "\n",
    "# X_train_temp = X_train.to_numpy().reshape(-1, 1)\n",
    "# y_train_temp = y_train.to_numpy().reshape(-1, 1)\n",
    "# X_test_temp = X_test.to_numpy().reshape(-1, 1)\n",
    "# y_test_temp = y_test.to_numpy().reshape(-1, 1)\n",
    "\n",
    "# clf2.fit(X_train_temp, y_train_temp)\n",
    "\n",
    "# print ('best solver:', clf.best_estimator_)\n",
    "# print ('best params:', clf.best_params_)\n",
    "# print ('best score:', clf.best_score_)\n",
    "# print ('scoring: ', clf.scoring)\n",
    "# print ('estimator:', clf.estimator)\n",
    "# print ('train score:', clf.score(X_train_temp, y_train_temp))\n",
    "# print ('test score:', clf.score(X_test_temp, y_test_temp))\n",
    "\n",
    "# y_pred = clf.predict(X_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc44fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temp.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d4dc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "params = {\n",
    "    'hidden_layer_sizes': [(10,), (20,), (30,),], # (30,),],\n",
    "    'activation': ['relu', 'logistic', ],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'alpha': [0.001, 0.003, 0.01, 0.03,], # 0.1, 0.3, 1.0,], # 3.0],\n",
    "    'learning_rate': ['adaptive'], #'constant', \n",
    "    'max_iter': [x for x in np.arange(100, 1001, 100)],\n",
    "    'random_state': [99,],\n",
    "    'tol': [1e-3, 1e-4,],\n",
    "    'early_stopping': [True, ],\n",
    "    'verbose': [True, ],\n",
    "#     'validation_fraction': [0.1, 0.2],\n",
    "    'validation_fraction': [0.1,],\n",
    "#     'n_iter_no_change': [10, 20],\n",
    "    'n_iter_no_change': [10,],\n",
    "    'learning_rate_init': [0.003,], #0.1, 0.001],\n",
    "}\n",
    "\n",
    "print (f\"Total number combinations for GridSearchCV is: { len(ParameterGrid(params)) }\")\n",
    "\n",
    "X_train_temp = X_train.to_numpy().reshape(-1, 1)\n",
    "y_train_temp = y_train.to_numpy().reshape(-1, 1).ravel()\n",
    "X_test_temp = X_test.to_numpy().reshape(-1, 1)\n",
    "y_test_temp = y_test.to_numpy().reshape(-1, 1).ravel()\n",
    "\n",
    "mlp = MLPRegressor()\n",
    "clf2 = GridSearchCV(mlp, params, n_jobs=-1)\n",
    "clf2.fit(X_train_temp, y_train_temp)\n",
    "\n",
    "print ('best solver:', clf.best_estimator_)\n",
    "print ('best params:', clf.best_params_)\n",
    "print ('best score:', clf.best_score_)\n",
    "print ('scoring: ', clf.scoring)\n",
    "print ('estimator:', clf.estimator)\n",
    "print ('train score:', clf.score(X_train_temp, y_train_temp))\n",
    "print ('test score:', clf.score(X_test_temp, y_test_temp))\n",
    "\n",
    "y_pred = clf.predict(X_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f04e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [x for x in np.arange(5,100,5)],\n",
    "    'n_jobs': [-1,],\n",
    "}\n",
    "\n",
    "X_train_temp = X_train.to_numpy().reshape(-1, 1)\n",
    "y_train_temp = y_train.to_numpy().reshape(-1, 1).ravel()\n",
    "X_test_temp = X_test.to_numpy().reshape(-1, 1)\n",
    "y_test_temp = y_test.to_numpy().reshape(-1, 1).ravel()\n",
    "\n",
    "rfr = RandomForestRegressor()\n",
    "clf4 = GridSearchCV(rfr, params)\n",
    "clf4.fit(X_train_temp, y_train_temp)\n",
    "\n",
    "print (f\"Total number combinations for GridSearchCV is: { len(ParameterGrid(params)) }\")\n",
    "\n",
    "print ('best solver:', clf4.best_estimator_)\n",
    "print ('best params:', clf4.best_params_)\n",
    "print ('best score:', clf4.best_score_)\n",
    "print ('scoring: ', clf4.scoring)\n",
    "print ('estimator:', clf4.estimator)\n",
    "print ('train score:', clf4.score(X_train_temp, y_train_temp))\n",
    "print ('test score:', clf4.score(X_test_temp, y_test_temp))\n",
    "\n",
    "y_pred = clf4.predict(X_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8389b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# dt = DecisionTreeRegressor()\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [x for x in np.arange(5,100,5)],\n",
    "    'n_jobs': [-1,],\n",
    "}\n",
    "\n",
    "dt = RandomForestRegressor()\n",
    "clf4 = GridSearchCV(dt, params)\n",
    "\n",
    "clf4.fit(X_train.to_numpy().reshape(-1,1) + (X_train.to_numpy()**2).reshape(-1,1) + (X_train.to_numpy()**3).reshape(-1,1) + (X_train.to_numpy()**4).reshape(-1,1), \\\n",
    "        y_train.to_numpy().reshape(-1,1) + (y_train.to_numpy()**2).reshape(-1,1) + (y_train.to_numpy()**3).reshape(-1,1) + (y_train.to_numpy()**4).reshape(-1,1)\n",
    "       )\n",
    "\n",
    "print (f\"Total number combinations for GridSearchCV is: { len(ParameterGrid(params)) }\")\n",
    "\n",
    "print ('best solver:', clf4.best_estimator_)\n",
    "print ('best params:', clf4.best_params_)\n",
    "print ('best score:', clf4.best_score_)\n",
    "print ('scoring: ', clf4.scoring)\n",
    "print ('estimator:', clf4.estimator)\n",
    "print ('train score:', clf4.score(X_train.to_numpy().reshape(-1,1) + (X_train.to_numpy()**2).reshape(-1,1) + (X_train.to_numpy()**3).reshape(-1,1) + (X_train.to_numpy()**4).reshape(-1,1), \\\n",
    "                y_train.to_numpy().reshape(-1,1) + (y_train.to_numpy()**2).reshape(-1,1) + (y_train.to_numpy()**3).reshape(-1,1) + (y_train.to_numpy()**4).reshape(-1,1)\n",
    "               ))\n",
    "print ('test score:', clf4.score(X_test.to_numpy().reshape(-1,1) + (X_test.to_numpy()**2).reshape(-1,1) + (X_test.to_numpy()**3).reshape(-1,1) + (X_test.to_numpy()**4).reshape(-1,1), \\\n",
    "                y_test.to_numpy().reshape(-1,1) + (y_test.to_numpy()**2).reshape(-1,1) + (y_test.to_numpy()**3).reshape(-1,1) + (y_test.to_numpy()**4).reshape(-1,1)\n",
    "               ))\n",
    "\n",
    "# y_pred = clf4.predict(X_test_temp)\n",
    "\n",
    "# print (clf4.score(X_train.to_numpy().reshape(-1,1) + (X_train.to_numpy()**2).reshape(-1,1) + (X_train.to_numpy()**3).reshape(-1,1) + (X_train.to_numpy()**4).reshape(-1,1), \\\n",
    "#                 y_train.to_numpy().reshape(-1,1) + (y_train.to_numpy()**2).reshape(-1,1) + (y_train.to_numpy()**3).reshape(-1,1) + (y_train.to_numpy()**4).reshape(-1,1)\n",
    "#                ))\n",
    "# print (clf4.score(X_test.to_numpy().reshape(-1,1) + (X_test.to_numpy()**2).reshape(-1,1) + (X_test.to_numpy()**3).reshape(-1,1) + (X_test.to_numpy()**4).reshape(-1,1), \\\n",
    "#                 y_test.to_numpy().reshape(-1,1) + (y_test.to_numpy()**2).reshape(-1,1) + (y_test.to_numpy()**3).reshape(-1,1) + (y_test.to_numpy()**4).reshape(-1,1)\n",
    "#                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9006f471",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "for i in range(1, 21):\n",
    "    poly_fetures_with_bias = PolynomialFeatures(degree=i, include_bias=True)\n",
    "    X_train_poly_bias = poly_fetures_with_bias.fit_transform(X_train.to_numpy().reshape(-1, 1))\n",
    "    X_test_poly_bias = poly_fetures_with_bias.fit_transform(X_test.to_numpy().reshape(-1, 1))\n",
    "    \n",
    "    poly_fetures_wo_bias = PolynomialFeatures(degree=i, include_bias=False)\n",
    "    X_train_poly_no_bias = poly_fetures_wo_bias.fit_transform(X_train.to_numpy().reshape(-1, 1))\n",
    "    X_test_poly_no_bias = poly_fetures_wo_bias.fit_transform(X_test.to_numpy().reshape(-1, 1))\n",
    "    \n",
    "    rf_bias = RandomForestRegressor(n_jobs=-1)\n",
    "    rf_reg_bias = rf_bias.fit(X_train_poly_bias, y_train.to_numpy().reshape(-1, 1))\n",
    "    \n",
    "    train_score_bias = rf_reg_bias.score(X_train_poly_bias, y_train.to_numpy().reshape(-1, 1))\n",
    "    test_score_bias = rf_reg_bias.score(X_test_poly_bias, y_test.to_numpy().reshape(-1, 1))\n",
    "    \n",
    "    rf_no_bias = RandomForestRegressor(n_jobs=-1)\n",
    "    rf_reg_bias = rf_no_bias.fit(X_train_poly_no_bias, y_train.to_numpy().reshape(-1, 1))\n",
    "    \n",
    "    train_score_no_bias = rf_reg_bias.score(X_train_poly_no_bias, y_train.to_numpy().reshape(-1, 1))\n",
    "    test_score_no_bias = rf_reg_bias.score(X_test_poly_no_bias, y_test.to_numpy().reshape(-1, 1))\n",
    "    \n",
    "    print (f\"degree: {i} train score: {train_score_bias} test score: {test_score_bias} with bias\")\n",
    "    print (f\"degree: {i} train score: {train_score_no_bias} test score: {test_score_no_bias} with no bias\")\n",
    "    print ('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f279c746",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [x for x in np.arange(1.0, 0.7, step=-0.003)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b73e6cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(10,5), dpi=100)\n",
    "# sns.scatterplot(X_train.to_numpy(), y_train.to_numpy())\n",
    "sns.scatterplot(X_train.to_numpy() + X_train.to_numpy()**2 + X_train.to_numpy()**3 + X_train.to_numpy()**4, \\\n",
    "                y_train.to_numpy())\n",
    "plt.xlabel(\"property_value\")\n",
    "plt.ylabel(\"loan_amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d851e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Before removing the features or rows lets check the relationship of all the columns with our dependent variable\n",
    "\n",
    "# plt.figure(figsize=(30, 20), dpi=100) \n",
    "\n",
    "# sns.heatmap(dataset.replace(np.nan,'').to_pandas().corr(), annot=True, annot_kws={\"fontsize\":12}, linecolor='white', \\\n",
    "#             linewidth=1, fmt='.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d46ac2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e379c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'alpha': [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0],\n",
    "    'solver': ['auto', 'svd', 'sag', 'saga', 'lbfgs'],\n",
    "    'max_iter': [x for x in np.arange(1000, 20000, step=1000)],\n",
    "    'tol': [1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n",
    "}\n",
    "\n",
    "ridge = Ridge()\n",
    "clf = GridSearchCV(ridge, params, n_jobs=-1)\n",
    "clf.fit(X_train.get().reshape(-1,1), y_train.get().reshape(-1,1))\n",
    "\n",
    "print ('best solver:', clf.best_estimator_)\n",
    "print ('best params:', clf.best_params_)\n",
    "print ('best score:', clf.best_score_)\n",
    "print ('scoring: ', clf.scoring)\n",
    "print ('estimator:', clf.estimator)\n",
    "print ('score:', clf.score(X_train.get().reshape(-1,1), y_train.get().reshape(-1,1)))\n",
    "\n",
    "# reg = ridge.fit(X_train.get().reshape(-1,1), y_train.get().reshape(-1,1))\n",
    "\n",
    "# reg.score(X_train.get().reshape(-1,1), y_train.get().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3419d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test.get().reshape(-1,1)) #  y_test.get().reshape(-1,1)\n",
    "# y_pred\n",
    "\n",
    "clf.score(X_test.get().reshape(-1,1), y_test.get().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c250babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving sklearn model\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "filename = './finalized_model.sav'\n",
    "if os.path.exists(filename):\n",
    "    os.remove(filename)\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c926cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "params = {\n",
    "    'penalty': ['l1', 'l2',],\n",
    "    'alpha': [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0],\n",
    "    'max_iter': [x for x in np.arange(50, 2001, 50)],\n",
    "    'tol': [1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n",
    "    'n_jobs': [-1,],\n",
    "    'random_state': [99,],\n",
    "    'early_stopping': [True, ],\n",
    "}\n",
    "\n",
    "p = Perceptron()\n",
    "clf2 = GridSearchCV(p, params)\n",
    "\n",
    "X_train_temp = X_train.get().astype(float).reshape(-1, 1)\n",
    "y_train_temp = y_train.get().astype(float).reshape(-1, 1)\n",
    "X_test_temp = X_test.get().astype(float).reshape(-1, 1)\n",
    "y_test_temp = y_test.get().astype(float).reshape(-1, 1)\n",
    "\n",
    "clf2.fit(X_train_temp, y_train_temp)\n",
    "\n",
    "print ('best solver:', clf.best_estimator_)\n",
    "print ('best params:', clf.best_params_)\n",
    "print ('best score:', clf.best_score_)\n",
    "print ('scoring: ', clf.scoring)\n",
    "print ('estimator:', clf.estimator)\n",
    "print ('train score:', clf.score(X_train_temp, y_train_temp))\n",
    "print ('test score:', clf.score(X_test_temp, y_test_temp))\n",
    "\n",
    "y_pred = clf.predict(X_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93bf35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temp.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c5b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "params = {\n",
    "    'hidden_layer_sizes': [(10,), (15,), (20,), (25,), (30,)],\n",
    "    'activation': ['relu', 'logistic', ],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'alpha': [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'max_iter': [x for x in np.arange(50, 1001, 50)],\n",
    "    'random_state': [99,],\n",
    "    'tol': [1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n",
    "    'verbose': [True, ],\n",
    "    'early_stopping': [True, ],\n",
    "    'validation_fraction': [0.1, 0.2],\n",
    "    'n_iter_no_change': [10, 20],\n",
    "}\n",
    "\n",
    "mlp = MLPRegressor()\n",
    "clf2 = GridSearchCV(mlp, params, n_jobs=-1)\n",
    "clf2.fit(X_train.get().reshape(-1,1), y_train.get().reshape(-1,1))\n",
    "\n",
    "print ('best solver:', clf.best_estimator_)\n",
    "print ('best params:', clf.best_params_)\n",
    "print ('best score:', clf.best_score_)\n",
    "print ('scoring: ', clf.scoring)\n",
    "print ('estimator:', clf.estimator)\n",
    "print ('train score:', clf.score(X_train.get().reshape(-1,1), y_train.get().reshape(-1,1)))\n",
    "print ('test score:', clf.score(X_test.get().reshape(-1,1), y_test.get().reshape(-1,1)))\n",
    "\n",
    "y_pred = clf.predict(X_test.get().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d65d02c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [x for x in np.arange(1.0, 0.7, step=-0.003)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd936d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Before removing the features or rows lets check the relationship of all the columns with our dependent variable\n",
    "\n",
    "# plt.figure(figsize=(30, 20), dpi=100) \n",
    "\n",
    "# sns.heatmap(dataset.replace(np.nan,'').to_pandas().corr(), annot=True, annot_kws={\"fontsize\":12}, linecolor='white', \\\n",
    "#             linewidth=1, fmt='.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfc8cd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now, we will perform LabelEncoding to categorize this variable to int value instead of object\n",
    "# Status column we don't need to categorize, it already has 0 and 1\n",
    "\n",
    "# But before we do that we need to remove all na values from dataset we don't those to be caterized as anything.\\\n",
    "    # That would be very misleading\n",
    "    \n",
    "# d2 = dataset.dropna(axis='rows', inplace=False) #.isna().sum()\n",
    "# d2\n",
    "\n",
    "for column in dataset.columns:\n",
    "    dataset = dataset[dataset[column].notna()]\n",
    "\n",
    "dataset\n",
    "# from cuml import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82607f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after dropping all rows containing na values we have following number of data \n",
    "\n",
    "new_n_rows, new_n_features = dataset.shape\n",
    "print (f\"features: {new_n_features}\")\n",
    "print (f\"rows: {new_n_rows}\")\n",
    "\n",
    "# We also need to make sure that we have enough data to test all the categories\n",
    "# Status is our dependent feature\n",
    "dataset.groupby(by=\"Status\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef811d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know from earlier that column Gender has [['Female', 'Joint', 'Male', 'Sex Not Available']] these \\\n",
    "    # four different categories, where 'Sex Not Available' is kind of nan. \n",
    "# We need to treat this problem as well, let's see how many data points have 'Sex Not Available'.\n",
    "filter = dataset['Gender'] != 'Sex Not Available'\n",
    "# dataset.loc[:, 'Gender'].where(filter, inplace=False)\n",
    "dataset = dataset[filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d654ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_n_rows, new_n_features = dataset.shape\n",
    "print (f\"features: {new_n_features}\")\n",
    "print (f\"rows: {new_n_rows}\")\n",
    "\n",
    "# We also need to make sure that we have enough data to test all the categories\n",
    "# Status is our dependent feature\n",
    "dataset.groupby(by=\"Status\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafea8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9709cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fabfbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_n_rows, new_n_features = dataset.shape\n",
    "print (f\"features: {new_n_features}\")\n",
    "print (f\"rows: {new_n_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d95a334",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Standard scalling was here column identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cda1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[scale_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82773cff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# standard scalling was here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c810c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[scale_columns].mean() #.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318351c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[scale_columns].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b338f01d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2583fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9960d835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cab65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(pip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7861a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Status\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a71bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c38bea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
