{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e4f93bc",
   "metadata": {},
   "source": [
    "# Kaggle Loan Defaulter Problemset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b17669",
   "metadata": {},
   "source": [
    "#### Here we will use Nvidia rapids libraries cuml, cudf to use grphics card instead CPU\n",
    "#### Let's import all the libraries required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69adcd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb2c9ca",
   "metadata": {},
   "source": [
    "#### Reading csv file to dataframe using cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0fd6a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>year</th>\n",
       "      <th>loan_limit</th>\n",
       "      <th>Gender</th>\n",
       "      <th>approv_in_adv</th>\n",
       "      <th>loan_type</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>Credit_Worthiness</th>\n",
       "      <th>open_credit</th>\n",
       "      <th>business_or_commercial</th>\n",
       "      <th>...</th>\n",
       "      <th>credit_type</th>\n",
       "      <th>Credit_Score</th>\n",
       "      <th>co-applicant_credit_type</th>\n",
       "      <th>age</th>\n",
       "      <th>submission_of_application</th>\n",
       "      <th>LTV</th>\n",
       "      <th>Region</th>\n",
       "      <th>Security_Type</th>\n",
       "      <th>Status</th>\n",
       "      <th>dtir1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24890</td>\n",
       "      <td>2019</td>\n",
       "      <td>cf</td>\n",
       "      <td>Sex Not Available</td>\n",
       "      <td>nopre</td>\n",
       "      <td>type1</td>\n",
       "      <td>p1</td>\n",
       "      <td>l1</td>\n",
       "      <td>nopc</td>\n",
       "      <td>nob/c</td>\n",
       "      <td>...</td>\n",
       "      <td>EXP</td>\n",
       "      <td>758</td>\n",
       "      <td>CIB</td>\n",
       "      <td>25-34</td>\n",
       "      <td>to_inst</td>\n",
       "      <td>98.72881356</td>\n",
       "      <td>south</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24891</td>\n",
       "      <td>2019</td>\n",
       "      <td>cf</td>\n",
       "      <td>Male</td>\n",
       "      <td>nopre</td>\n",
       "      <td>type2</td>\n",
       "      <td>p1</td>\n",
       "      <td>l1</td>\n",
       "      <td>nopc</td>\n",
       "      <td>b/c</td>\n",
       "      <td>...</td>\n",
       "      <td>EQUI</td>\n",
       "      <td>552</td>\n",
       "      <td>EXP</td>\n",
       "      <td>55-64</td>\n",
       "      <td>to_inst</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>North</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24892</td>\n",
       "      <td>2019</td>\n",
       "      <td>cf</td>\n",
       "      <td>Male</td>\n",
       "      <td>pre</td>\n",
       "      <td>type1</td>\n",
       "      <td>p1</td>\n",
       "      <td>l1</td>\n",
       "      <td>nopc</td>\n",
       "      <td>nob/c</td>\n",
       "      <td>...</td>\n",
       "      <td>EXP</td>\n",
       "      <td>834</td>\n",
       "      <td>CIB</td>\n",
       "      <td>35-44</td>\n",
       "      <td>to_inst</td>\n",
       "      <td>80.01968504</td>\n",
       "      <td>south</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24893</td>\n",
       "      <td>2019</td>\n",
       "      <td>cf</td>\n",
       "      <td>Male</td>\n",
       "      <td>nopre</td>\n",
       "      <td>type1</td>\n",
       "      <td>p4</td>\n",
       "      <td>l1</td>\n",
       "      <td>nopc</td>\n",
       "      <td>nob/c</td>\n",
       "      <td>...</td>\n",
       "      <td>EXP</td>\n",
       "      <td>587</td>\n",
       "      <td>CIB</td>\n",
       "      <td>45-54</td>\n",
       "      <td>not_inst</td>\n",
       "      <td>69.3768997</td>\n",
       "      <td>North</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24894</td>\n",
       "      <td>2019</td>\n",
       "      <td>cf</td>\n",
       "      <td>Joint</td>\n",
       "      <td>pre</td>\n",
       "      <td>type1</td>\n",
       "      <td>p1</td>\n",
       "      <td>l1</td>\n",
       "      <td>nopc</td>\n",
       "      <td>nob/c</td>\n",
       "      <td>...</td>\n",
       "      <td>CRIF</td>\n",
       "      <td>602</td>\n",
       "      <td>EXP</td>\n",
       "      <td>25-34</td>\n",
       "      <td>not_inst</td>\n",
       "      <td>91.88654354</td>\n",
       "      <td>North</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  year loan_limit             Gender approv_in_adv loan_type  \\\n",
       "0  24890  2019         cf  Sex Not Available         nopre     type1   \n",
       "1  24891  2019         cf               Male         nopre     type2   \n",
       "2  24892  2019         cf               Male           pre     type1   \n",
       "3  24893  2019         cf               Male         nopre     type1   \n",
       "4  24894  2019         cf              Joint           pre     type1   \n",
       "\n",
       "  loan_purpose Credit_Worthiness open_credit business_or_commercial  ...  \\\n",
       "0           p1                l1        nopc                  nob/c  ...   \n",
       "1           p1                l1        nopc                    b/c  ...   \n",
       "2           p1                l1        nopc                  nob/c  ...   \n",
       "3           p4                l1        nopc                  nob/c  ...   \n",
       "4           p1                l1        nopc                  nob/c  ...   \n",
       "\n",
       "   credit_type Credit_Score  co-applicant_credit_type    age  \\\n",
       "0          EXP          758                       CIB  25-34   \n",
       "1         EQUI          552                       EXP  55-64   \n",
       "2          EXP          834                       CIB  35-44   \n",
       "3          EXP          587                       CIB  45-54   \n",
       "4         CRIF          602                       EXP  25-34   \n",
       "\n",
       "  submission_of_application          LTV Region Security_Type Status  dtir1  \n",
       "0                   to_inst  98.72881356  south        direct      1   45.0  \n",
       "1                   to_inst         <NA>  North        direct      1   <NA>  \n",
       "2                   to_inst  80.01968504  south        direct      0   46.0  \n",
       "3                  not_inst   69.3768997  North        direct      0   42.0  \n",
       "4                  not_inst  91.88654354  North        direct      0   39.0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = cudf.read_csv('./Loan_Default.csv')\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af67008f",
   "metadata": {},
   "source": [
    "### There are 34 feature vectors in this dataset and DataFrame is not displaying all of them. To solve this we can set number of columns we want to display with pandas settings as shown in the cell below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "312f345b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>year</th>\n",
       "      <th>loan_limit</th>\n",
       "      <th>Gender</th>\n",
       "      <th>approv_in_adv</th>\n",
       "      <th>loan_type</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>Credit_Worthiness</th>\n",
       "      <th>open_credit</th>\n",
       "      <th>business_or_commercial</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>rate_of_interest</th>\n",
       "      <th>Interest_rate_spread</th>\n",
       "      <th>Upfront_charges</th>\n",
       "      <th>term</th>\n",
       "      <th>Neg_ammortization</th>\n",
       "      <th>interest_only</th>\n",
       "      <th>lump_sum_payment</th>\n",
       "      <th>property_value</th>\n",
       "      <th>construction_type</th>\n",
       "      <th>occupancy_type</th>\n",
       "      <th>Secured_by</th>\n",
       "      <th>total_units</th>\n",
       "      <th>income</th>\n",
       "      <th>credit_type</th>\n",
       "      <th>Credit_Score</th>\n",
       "      <th>co-applicant_credit_type</th>\n",
       "      <th>age</th>\n",
       "      <th>submission_of_application</th>\n",
       "      <th>LTV</th>\n",
       "      <th>Region</th>\n",
       "      <th>Security_Type</th>\n",
       "      <th>Status</th>\n",
       "      <th>dtir1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24890</td>\n",
       "      <td>2019</td>\n",
       "      <td>cf</td>\n",
       "      <td>Sex Not Available</td>\n",
       "      <td>nopre</td>\n",
       "      <td>type1</td>\n",
       "      <td>p1</td>\n",
       "      <td>l1</td>\n",
       "      <td>nopc</td>\n",
       "      <td>nob/c</td>\n",
       "      <td>116500</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>360.0</td>\n",
       "      <td>not_neg</td>\n",
       "      <td>not_int</td>\n",
       "      <td>not_lpsm</td>\n",
       "      <td>118000.0</td>\n",
       "      <td>sb</td>\n",
       "      <td>pr</td>\n",
       "      <td>home</td>\n",
       "      <td>1U</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>EXP</td>\n",
       "      <td>758</td>\n",
       "      <td>CIB</td>\n",
       "      <td>25-34</td>\n",
       "      <td>to_inst</td>\n",
       "      <td>98.72881356</td>\n",
       "      <td>south</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24891</td>\n",
       "      <td>2019</td>\n",
       "      <td>cf</td>\n",
       "      <td>Male</td>\n",
       "      <td>nopre</td>\n",
       "      <td>type2</td>\n",
       "      <td>p1</td>\n",
       "      <td>l1</td>\n",
       "      <td>nopc</td>\n",
       "      <td>b/c</td>\n",
       "      <td>206500</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>360.0</td>\n",
       "      <td>not_neg</td>\n",
       "      <td>not_int</td>\n",
       "      <td>lpsm</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>sb</td>\n",
       "      <td>pr</td>\n",
       "      <td>home</td>\n",
       "      <td>1U</td>\n",
       "      <td>4980.0</td>\n",
       "      <td>EQUI</td>\n",
       "      <td>552</td>\n",
       "      <td>EXP</td>\n",
       "      <td>55-64</td>\n",
       "      <td>to_inst</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>North</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24892</td>\n",
       "      <td>2019</td>\n",
       "      <td>cf</td>\n",
       "      <td>Male</td>\n",
       "      <td>pre</td>\n",
       "      <td>type1</td>\n",
       "      <td>p1</td>\n",
       "      <td>l1</td>\n",
       "      <td>nopc</td>\n",
       "      <td>nob/c</td>\n",
       "      <td>406500</td>\n",
       "      <td>4.56</td>\n",
       "      <td>0.2</td>\n",
       "      <td>595.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>neg_amm</td>\n",
       "      <td>not_int</td>\n",
       "      <td>not_lpsm</td>\n",
       "      <td>508000.0</td>\n",
       "      <td>sb</td>\n",
       "      <td>pr</td>\n",
       "      <td>home</td>\n",
       "      <td>1U</td>\n",
       "      <td>9480.0</td>\n",
       "      <td>EXP</td>\n",
       "      <td>834</td>\n",
       "      <td>CIB</td>\n",
       "      <td>35-44</td>\n",
       "      <td>to_inst</td>\n",
       "      <td>80.01968504</td>\n",
       "      <td>south</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24893</td>\n",
       "      <td>2019</td>\n",
       "      <td>cf</td>\n",
       "      <td>Male</td>\n",
       "      <td>nopre</td>\n",
       "      <td>type1</td>\n",
       "      <td>p4</td>\n",
       "      <td>l1</td>\n",
       "      <td>nopc</td>\n",
       "      <td>nob/c</td>\n",
       "      <td>456500</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.681</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>360.0</td>\n",
       "      <td>not_neg</td>\n",
       "      <td>not_int</td>\n",
       "      <td>not_lpsm</td>\n",
       "      <td>658000.0</td>\n",
       "      <td>sb</td>\n",
       "      <td>pr</td>\n",
       "      <td>home</td>\n",
       "      <td>1U</td>\n",
       "      <td>11880.0</td>\n",
       "      <td>EXP</td>\n",
       "      <td>587</td>\n",
       "      <td>CIB</td>\n",
       "      <td>45-54</td>\n",
       "      <td>not_inst</td>\n",
       "      <td>69.3768997</td>\n",
       "      <td>North</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24894</td>\n",
       "      <td>2019</td>\n",
       "      <td>cf</td>\n",
       "      <td>Joint</td>\n",
       "      <td>pre</td>\n",
       "      <td>type1</td>\n",
       "      <td>p1</td>\n",
       "      <td>l1</td>\n",
       "      <td>nopc</td>\n",
       "      <td>nob/c</td>\n",
       "      <td>696500</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.3042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>not_neg</td>\n",
       "      <td>not_int</td>\n",
       "      <td>not_lpsm</td>\n",
       "      <td>758000.0</td>\n",
       "      <td>sb</td>\n",
       "      <td>pr</td>\n",
       "      <td>home</td>\n",
       "      <td>1U</td>\n",
       "      <td>10440.0</td>\n",
       "      <td>CRIF</td>\n",
       "      <td>602</td>\n",
       "      <td>EXP</td>\n",
       "      <td>25-34</td>\n",
       "      <td>not_inst</td>\n",
       "      <td>91.88654354</td>\n",
       "      <td>North</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  year loan_limit             Gender approv_in_adv loan_type  \\\n",
       "0  24890  2019         cf  Sex Not Available         nopre     type1   \n",
       "1  24891  2019         cf               Male         nopre     type2   \n",
       "2  24892  2019         cf               Male           pre     type1   \n",
       "3  24893  2019         cf               Male         nopre     type1   \n",
       "4  24894  2019         cf              Joint           pre     type1   \n",
       "\n",
       "  loan_purpose Credit_Worthiness open_credit business_or_commercial  \\\n",
       "0           p1                l1        nopc                  nob/c   \n",
       "1           p1                l1        nopc                    b/c   \n",
       "2           p1                l1        nopc                  nob/c   \n",
       "3           p4                l1        nopc                  nob/c   \n",
       "4           p1                l1        nopc                  nob/c   \n",
       "\n",
       "   loan_amount rate_of_interest Interest_rate_spread Upfront_charges   term  \\\n",
       "0       116500             <NA>                 <NA>            <NA>  360.0   \n",
       "1       206500             <NA>                 <NA>            <NA>  360.0   \n",
       "2       406500             4.56                  0.2           595.0  360.0   \n",
       "3       456500             4.25                0.681            <NA>  360.0   \n",
       "4       696500              4.0               0.3042             0.0  360.0   \n",
       "\n",
       "  Neg_ammortization interest_only lump_sum_payment property_value  \\\n",
       "0           not_neg       not_int         not_lpsm       118000.0   \n",
       "1           not_neg       not_int             lpsm           <NA>   \n",
       "2           neg_amm       not_int         not_lpsm       508000.0   \n",
       "3           not_neg       not_int         not_lpsm       658000.0   \n",
       "4           not_neg       not_int         not_lpsm       758000.0   \n",
       "\n",
       "  construction_type occupancy_type Secured_by total_units   income  \\\n",
       "0                sb             pr       home          1U   1740.0   \n",
       "1                sb             pr       home          1U   4980.0   \n",
       "2                sb             pr       home          1U   9480.0   \n",
       "3                sb             pr       home          1U  11880.0   \n",
       "4                sb             pr       home          1U  10440.0   \n",
       "\n",
       "  credit_type  Credit_Score co-applicant_credit_type    age  \\\n",
       "0         EXP           758                      CIB  25-34   \n",
       "1        EQUI           552                      EXP  55-64   \n",
       "2         EXP           834                      CIB  35-44   \n",
       "3         EXP           587                      CIB  45-54   \n",
       "4        CRIF           602                      EXP  25-34   \n",
       "\n",
       "  submission_of_application          LTV Region Security_Type  Status dtir1  \n",
       "0                   to_inst  98.72881356  south        direct       1  45.0  \n",
       "1                   to_inst         <NA>  North        direct       1  <NA>  \n",
       "2                   to_inst  80.01968504  south        direct       0  46.0  \n",
       "3                  not_inst   69.3768997  North        direct       0  42.0  \n",
       "4                  not_inst  91.88654354  North        direct       0  39.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_columns\", dataset.shape[-1]) \n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f6c17d",
   "metadata": {},
   "source": [
    "### Feature vector ID is not useful, we can just drop it and if we have data of only year then it's not a timeseries problem, we can drop that as well. Let's check how many unique values we have in the year feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74dc5938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2019\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['year'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8c49a5",
   "metadata": {},
   "source": [
    "### This data is only for the year 2019, it's same for all the vector this we can drop this as well.\n",
    "### In the cell below we will drop ID, and year feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e3c4fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_limit</th>\n",
       "      <th>Gender</th>\n",
       "      <th>approv_in_adv</th>\n",
       "      <th>loan_type</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>Credit_Worthiness</th>\n",
       "      <th>open_credit</th>\n",
       "      <th>business_or_commercial</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>rate_of_interest</th>\n",
       "      <th>Interest_rate_spread</th>\n",
       "      <th>Upfront_charges</th>\n",
       "      <th>term</th>\n",
       "      <th>Neg_ammortization</th>\n",
       "      <th>interest_only</th>\n",
       "      <th>lump_sum_payment</th>\n",
       "      <th>property_value</th>\n",
       "      <th>construction_type</th>\n",
       "      <th>occupancy_type</th>\n",
       "      <th>Secured_by</th>\n",
       "      <th>total_units</th>\n",
       "      <th>income</th>\n",
       "      <th>credit_type</th>\n",
       "      <th>Credit_Score</th>\n",
       "      <th>co-applicant_credit_type</th>\n",
       "      <th>age</th>\n",
       "      <th>submission_of_application</th>\n",
       "      <th>LTV</th>\n",
       "      <th>Region</th>\n",
       "      <th>Security_Type</th>\n",
       "      <th>Status</th>\n",
       "      <th>dtir1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cf</td>\n",
       "      <td>Sex Not Available</td>\n",
       "      <td>nopre</td>\n",
       "      <td>type1</td>\n",
       "      <td>p1</td>\n",
       "      <td>l1</td>\n",
       "      <td>nopc</td>\n",
       "      <td>nob/c</td>\n",
       "      <td>116500</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>360.0</td>\n",
       "      <td>not_neg</td>\n",
       "      <td>not_int</td>\n",
       "      <td>not_lpsm</td>\n",
       "      <td>118000.0</td>\n",
       "      <td>sb</td>\n",
       "      <td>pr</td>\n",
       "      <td>home</td>\n",
       "      <td>1U</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>EXP</td>\n",
       "      <td>758</td>\n",
       "      <td>CIB</td>\n",
       "      <td>25-34</td>\n",
       "      <td>to_inst</td>\n",
       "      <td>98.72881356</td>\n",
       "      <td>south</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cf</td>\n",
       "      <td>Male</td>\n",
       "      <td>nopre</td>\n",
       "      <td>type2</td>\n",
       "      <td>p1</td>\n",
       "      <td>l1</td>\n",
       "      <td>nopc</td>\n",
       "      <td>b/c</td>\n",
       "      <td>206500</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>360.0</td>\n",
       "      <td>not_neg</td>\n",
       "      <td>not_int</td>\n",
       "      <td>lpsm</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>sb</td>\n",
       "      <td>pr</td>\n",
       "      <td>home</td>\n",
       "      <td>1U</td>\n",
       "      <td>4980.0</td>\n",
       "      <td>EQUI</td>\n",
       "      <td>552</td>\n",
       "      <td>EXP</td>\n",
       "      <td>55-64</td>\n",
       "      <td>to_inst</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>North</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cf</td>\n",
       "      <td>Male</td>\n",
       "      <td>pre</td>\n",
       "      <td>type1</td>\n",
       "      <td>p1</td>\n",
       "      <td>l1</td>\n",
       "      <td>nopc</td>\n",
       "      <td>nob/c</td>\n",
       "      <td>406500</td>\n",
       "      <td>4.56</td>\n",
       "      <td>0.2</td>\n",
       "      <td>595.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>neg_amm</td>\n",
       "      <td>not_int</td>\n",
       "      <td>not_lpsm</td>\n",
       "      <td>508000.0</td>\n",
       "      <td>sb</td>\n",
       "      <td>pr</td>\n",
       "      <td>home</td>\n",
       "      <td>1U</td>\n",
       "      <td>9480.0</td>\n",
       "      <td>EXP</td>\n",
       "      <td>834</td>\n",
       "      <td>CIB</td>\n",
       "      <td>35-44</td>\n",
       "      <td>to_inst</td>\n",
       "      <td>80.01968504</td>\n",
       "      <td>south</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cf</td>\n",
       "      <td>Male</td>\n",
       "      <td>nopre</td>\n",
       "      <td>type1</td>\n",
       "      <td>p4</td>\n",
       "      <td>l1</td>\n",
       "      <td>nopc</td>\n",
       "      <td>nob/c</td>\n",
       "      <td>456500</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.681</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>360.0</td>\n",
       "      <td>not_neg</td>\n",
       "      <td>not_int</td>\n",
       "      <td>not_lpsm</td>\n",
       "      <td>658000.0</td>\n",
       "      <td>sb</td>\n",
       "      <td>pr</td>\n",
       "      <td>home</td>\n",
       "      <td>1U</td>\n",
       "      <td>11880.0</td>\n",
       "      <td>EXP</td>\n",
       "      <td>587</td>\n",
       "      <td>CIB</td>\n",
       "      <td>45-54</td>\n",
       "      <td>not_inst</td>\n",
       "      <td>69.3768997</td>\n",
       "      <td>North</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cf</td>\n",
       "      <td>Joint</td>\n",
       "      <td>pre</td>\n",
       "      <td>type1</td>\n",
       "      <td>p1</td>\n",
       "      <td>l1</td>\n",
       "      <td>nopc</td>\n",
       "      <td>nob/c</td>\n",
       "      <td>696500</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.3042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>not_neg</td>\n",
       "      <td>not_int</td>\n",
       "      <td>not_lpsm</td>\n",
       "      <td>758000.0</td>\n",
       "      <td>sb</td>\n",
       "      <td>pr</td>\n",
       "      <td>home</td>\n",
       "      <td>1U</td>\n",
       "      <td>10440.0</td>\n",
       "      <td>CRIF</td>\n",
       "      <td>602</td>\n",
       "      <td>EXP</td>\n",
       "      <td>25-34</td>\n",
       "      <td>not_inst</td>\n",
       "      <td>91.88654354</td>\n",
       "      <td>North</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  loan_limit             Gender approv_in_adv loan_type loan_purpose  \\\n",
       "0         cf  Sex Not Available         nopre     type1           p1   \n",
       "1         cf               Male         nopre     type2           p1   \n",
       "2         cf               Male           pre     type1           p1   \n",
       "3         cf               Male         nopre     type1           p4   \n",
       "4         cf              Joint           pre     type1           p1   \n",
       "\n",
       "  Credit_Worthiness open_credit business_or_commercial  loan_amount  \\\n",
       "0                l1        nopc                  nob/c       116500   \n",
       "1                l1        nopc                    b/c       206500   \n",
       "2                l1        nopc                  nob/c       406500   \n",
       "3                l1        nopc                  nob/c       456500   \n",
       "4                l1        nopc                  nob/c       696500   \n",
       "\n",
       "  rate_of_interest Interest_rate_spread Upfront_charges   term  \\\n",
       "0             <NA>                 <NA>            <NA>  360.0   \n",
       "1             <NA>                 <NA>            <NA>  360.0   \n",
       "2             4.56                  0.2           595.0  360.0   \n",
       "3             4.25                0.681            <NA>  360.0   \n",
       "4              4.0               0.3042             0.0  360.0   \n",
       "\n",
       "  Neg_ammortization interest_only lump_sum_payment property_value  \\\n",
       "0           not_neg       not_int         not_lpsm       118000.0   \n",
       "1           not_neg       not_int             lpsm           <NA>   \n",
       "2           neg_amm       not_int         not_lpsm       508000.0   \n",
       "3           not_neg       not_int         not_lpsm       658000.0   \n",
       "4           not_neg       not_int         not_lpsm       758000.0   \n",
       "\n",
       "  construction_type occupancy_type Secured_by total_units   income  \\\n",
       "0                sb             pr       home          1U   1740.0   \n",
       "1                sb             pr       home          1U   4980.0   \n",
       "2                sb             pr       home          1U   9480.0   \n",
       "3                sb             pr       home          1U  11880.0   \n",
       "4                sb             pr       home          1U  10440.0   \n",
       "\n",
       "  credit_type  Credit_Score co-applicant_credit_type    age  \\\n",
       "0         EXP           758                      CIB  25-34   \n",
       "1        EQUI           552                      EXP  55-64   \n",
       "2         EXP           834                      CIB  35-44   \n",
       "3         EXP           587                      CIB  45-54   \n",
       "4        CRIF           602                      EXP  25-34   \n",
       "\n",
       "  submission_of_application          LTV Region Security_Type  Status dtir1  \n",
       "0                   to_inst  98.72881356  south        direct       1  45.0  \n",
       "1                   to_inst         <NA>  North        direct       1  <NA>  \n",
       "2                   to_inst  80.01968504  south        direct       0  46.0  \n",
       "3                  not_inst   69.3768997  North        direct       0  42.0  \n",
       "4                  not_inst  91.88654354  North        direct       0  39.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# axis=1 = columns and 0 = rows\n",
    "dataset.drop(['ID', 'year'], axis=1, inplace=True)\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debe6c98",
   "metadata": {},
   "source": [
    "### We want to be sure that our dataset is not biased and we have enough number of records for all categories in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f11b64",
   "metadata": {},
   "source": [
    "### In the cell below we'll plot the barplot to check the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29464575",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset has features: 32\n",
      "Total number of samples in the dataset rows: 148670\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_rows, n_features = dataset.shape\n",
    "print (f\"This dataset has features: {n_features}\")\n",
    "print (f\"Total number of samples in the dataset rows: {n_rows}\")\n",
    "\n",
    "# We also need to make sure that we have enough data to test all the categories\n",
    "# Status is our dependent feature\n",
    "dist = dataset.groupby(by=\"Status\").size()\n",
    "\n",
    "_ = sns.barplot(x=dist.index.values_host, y=dist.values.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6148693",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "1. The dataset has lot of values for class 0 and compared to that very few values for the class 1. But this should be enough for training the model. But,\n",
    "2. While splitting the dataset to train and test, we need to make sure that we have enough number of class 1 samples in the test array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81000d98",
   "metadata": {},
   "source": [
    "## Checking the null values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a13ab84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of null values in the class 0: 7911\n",
      "Maximum number of null values in the class 1: 36639\n"
     ]
    }
   ],
   "source": [
    "null_cols_status_0 = dataset[dataset[\"Status\"] == 0].isnull().sum() # we can also use isnull method both are same\n",
    "null_cols_status_1 = dataset[dataset[\"Status\"] == 1].isnull().sum() # we can also use isnull method both are same\n",
    "\n",
    "print (f\"Maximum number of null values in the class 0: {np.max(null_cols_status_0)}\")\n",
    "print (f\"Maximum number of null values in the class 1: {np.max(null_cols_status_1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ad592",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "1. For the class 0 we have 7,911 null values, if we dropped all of these it would be fine because we still will have enough number of samples to train the model. But,\n",
    "2. For the class 1 we maybe have just enough data to train our model and we can't just drop all the 36,639 rows.\n",
    "3. We need to replace the null values with some statistically appropriate data for all the feture vector with null values in them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b5506a",
   "metadata": {},
   "source": [
    "### In the following cell we will get the all the feature names which has null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5c7d2c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dtir1',\n",
       " 'approv_in_adv',\n",
       " 'age',\n",
       " 'Upfront_charges',\n",
       " 'Interest_rate_spread',\n",
       " 'loan_limit',\n",
       " 'income',\n",
       " 'loan_purpose',\n",
       " 'property_value',\n",
       " 'Neg_ammortization',\n",
       " 'rate_of_interest',\n",
       " 'LTV',\n",
       " 'submission_of_application',\n",
       " 'term']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp0 = set(null_cols_status_0.index[null_cols_status_0 > 0].values_host)\n",
    "temp1 = set(null_cols_status_0.index[null_cols_status_1 > 0].values_host)\n",
    "\n",
    "cols_with_na_values = list(temp0.union(temp1))\n",
    "del temp0, temp1\n",
    "\n",
    "cols_with_na_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510bb25f",
   "metadata": {},
   "source": [
    "## Before performing any statistical operatings we need to see that all the features are stored in the DataFrame with appropriate datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "013426b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'cudf.core.dataframe.DataFrame'>\n",
      "RangeIndex: 148670 entries, 0 to 148669\n",
      "Data columns (total 32 columns):\n",
      " #   Column                     Non-Null Count   Dtype\n",
      "---  ------                     --------------   -----\n",
      " 0   loan_limit                 145326 non-null  object\n",
      " 1   Gender                     148670 non-null  object\n",
      " 2   approv_in_adv              147762 non-null  object\n",
      " 3   loan_type                  148670 non-null  object\n",
      " 4   loan_purpose               148536 non-null  object\n",
      " 5   Credit_Worthiness          148670 non-null  object\n",
      " 6   open_credit                148670 non-null  object\n",
      " 7   business_or_commercial     148670 non-null  object\n",
      " 8   loan_amount                148670 non-null  int64\n",
      " 9   rate_of_interest           112231 non-null  float64\n",
      " 10  Interest_rate_spread       112031 non-null  float64\n",
      " 11  Upfront_charges            109028 non-null  float64\n",
      " 12  term                       148629 non-null  float64\n",
      " 13  Neg_ammortization          148549 non-null  object\n",
      " 14  interest_only              148670 non-null  object\n",
      " 15  lump_sum_payment           148670 non-null  object\n",
      " 16  property_value             133572 non-null  float64\n",
      " 17  construction_type          148670 non-null  object\n",
      " 18  occupancy_type             148670 non-null  object\n",
      " 19  Secured_by                 148670 non-null  object\n",
      " 20  total_units                148670 non-null  object\n",
      " 21  income                     139520 non-null  float64\n",
      " 22  credit_type                148670 non-null  object\n",
      " 23  Credit_Score               148670 non-null  int64\n",
      " 24  co-applicant_credit_type   148670 non-null  object\n",
      " 25  age                        148470 non-null  object\n",
      " 26  submission_of_application  148470 non-null  object\n",
      " 27  LTV                        133572 non-null  float64\n",
      " 28  Region                     148670 non-null  object\n",
      " 29  Security_Type              148670 non-null  object\n",
      " 30  Status                     148670 non-null  int64\n",
      " 31  dtir1                      124549 non-null  float64\n",
      "dtypes: float64(8), int64(3), object(21)\n",
      "memory usage: 38.0+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b288b8",
   "metadata": {},
   "source": [
    "### Obervations:\n",
    "1. As you can see lots of features are stored with obeject datatype, in this case it is not necessary that all of them are having string values but some integer or float values may be are stored as string in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300058b0",
   "metadata": {},
   "source": [
    "## Let's check what kind of values all the feature vectors have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f184067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    <NA>\n",
      "1      cf\n",
      "2     ncf\n",
      "Name: loan_limit, dtype: object\n",
      "0               Female\n",
      "1                Joint\n",
      "2                 Male\n",
      "3    Sex Not Available\n",
      "Name: Gender, dtype: object\n",
      "0     <NA>\n",
      "1    nopre\n",
      "2      pre\n",
      "Name: approv_in_adv, dtype: object\n",
      "0    type1\n",
      "1    type2\n",
      "2    type3\n",
      "Name: loan_type, dtype: object\n",
      "0    <NA>\n",
      "1      p1\n",
      "2      p2\n",
      "3      p3\n",
      "4      p4\n",
      "Name: loan_purpose, dtype: object\n",
      "0    l1\n",
      "1    l2\n",
      "Name: Credit_Worthiness, dtype: object\n",
      "0    nopc\n",
      "1     opc\n",
      "Name: open_credit, dtype: object\n",
      "0      b/c\n",
      "1    nob/c\n",
      "Name: business_or_commercial, dtype: object\n",
      "0        16500\n",
      "1        26500\n",
      "2        36500\n",
      "3        46500\n",
      "4        56500\n",
      "        ...   \n",
      "206    2926500\n",
      "207    2986500\n",
      "208    3006500\n",
      "209    3346500\n",
      "210    3576500\n",
      "Name: loan_amount, Length: 211, dtype: int64\n",
      "0       <NA>\n",
      "1        0.0\n",
      "2      2.125\n",
      "3       2.25\n",
      "4      2.375\n",
      "       ...  \n",
      "127    7.125\n",
      "128    7.375\n",
      "129      7.5\n",
      "130     7.75\n",
      "131      8.0\n",
      "Name: rate_of_interest, Length: 132, dtype: float64\n",
      "0           <NA>\n",
      "1         -3.638\n",
      "2        -1.0841\n",
      "3         -1.047\n",
      "4        -1.0462\n",
      "          ...   \n",
      "22512     2.5932\n",
      "22513     2.6368\n",
      "22514     2.7227\n",
      "22515     2.8854\n",
      "22516      3.357\n",
      "Name: Interest_rate_spread, Length: 22517, dtype: float64\n",
      "0            <NA>\n",
      "1             0.0\n",
      "2            0.03\n",
      "3            0.06\n",
      "4            0.35\n",
      "           ...   \n",
      "58267    37604.38\n",
      "58268     38375.0\n",
      "58269     38437.5\n",
      "58270    53485.78\n",
      "58271     60000.0\n",
      "Name: Upfront_charges, Length: 58272, dtype: float64\n",
      "0      <NA>\n",
      "1      96.0\n",
      "2     108.0\n",
      "3     120.0\n",
      "4     132.0\n",
      "5     144.0\n",
      "6     156.0\n",
      "7     165.0\n",
      "8     168.0\n",
      "9     180.0\n",
      "10    192.0\n",
      "11    204.0\n",
      "12    216.0\n",
      "13    228.0\n",
      "14    240.0\n",
      "15    252.0\n",
      "16    264.0\n",
      "17    276.0\n",
      "18    280.0\n",
      "19    288.0\n",
      "20    300.0\n",
      "21    312.0\n",
      "22    322.0\n",
      "23    324.0\n",
      "24    336.0\n",
      "25    348.0\n",
      "26    360.0\n",
      "Name: term, dtype: float64\n",
      "0       <NA>\n",
      "1    neg_amm\n",
      "2    not_neg\n",
      "Name: Neg_ammortization, dtype: object\n",
      "0    int_only\n",
      "1     not_int\n",
      "Name: interest_only, dtype: object\n",
      "0        lpsm\n",
      "1    not_lpsm\n",
      "Name: lump_sum_payment, dtype: object\n",
      "0            <NA>\n",
      "1          8000.0\n",
      "2         18000.0\n",
      "3         28000.0\n",
      "4         38000.0\n",
      "          ...    \n",
      "381     9268000.0\n",
      "382    10008000.0\n",
      "383    11008000.0\n",
      "384    12008000.0\n",
      "385    16508000.0\n",
      "Name: property_value, Length: 386, dtype: float64\n",
      "0    mh\n",
      "1    sb\n",
      "Name: construction_type, dtype: object\n",
      "0    ir\n",
      "1    pr\n",
      "2    sr\n",
      "Name: occupancy_type, dtype: object\n",
      "0    home\n",
      "1    land\n",
      "Name: Secured_by, dtype: object\n",
      "0    1U\n",
      "1    2U\n",
      "2    3U\n",
      "3    4U\n",
      "Name: total_units, dtype: object\n",
      "0           <NA>\n",
      "1            0.0\n",
      "2           60.0\n",
      "3          120.0\n",
      "4          180.0\n",
      "          ...   \n",
      "997     329460.0\n",
      "998     335880.0\n",
      "999     374400.0\n",
      "1000    377220.0\n",
      "1001    578580.0\n",
      "Name: income, Length: 1002, dtype: float64\n",
      "0     CIB\n",
      "1    CRIF\n",
      "2    EQUI\n",
      "3     EXP\n",
      "Name: credit_type, dtype: object\n",
      "0      500\n",
      "1      501\n",
      "2      502\n",
      "3      503\n",
      "4      504\n",
      "      ... \n",
      "396    896\n",
      "397    897\n",
      "398    898\n",
      "399    899\n",
      "400    900\n",
      "Name: Credit_Score, Length: 401, dtype: int64\n",
      "0    CIB\n",
      "1    EXP\n",
      "Name: co-applicant_credit_type, dtype: object\n",
      "0     <NA>\n",
      "1    25-34\n",
      "2    35-44\n",
      "3    45-54\n",
      "4    55-64\n",
      "5    65-74\n",
      "6      <25\n",
      "7      >74\n",
      "Name: age, dtype: object\n",
      "0        <NA>\n",
      "1    not_inst\n",
      "2     to_inst\n",
      "Name: submission_of_application, dtype: object\n",
      "0              <NA>\n",
      "1       0.967478198\n",
      "2       2.072942643\n",
      "3       2.767587397\n",
      "4        2.81374502\n",
      "           ...     \n",
      "8480        2956.25\n",
      "8481        4706.25\n",
      "8482        5206.25\n",
      "8483        6706.25\n",
      "8484        7831.25\n",
      "Name: LTV, Length: 8485, dtype: float64\n",
      "0         North\n",
      "1    North-East\n",
      "2       central\n",
      "3         south\n",
      "Name: Region, dtype: object\n",
      "0    Indriect\n",
      "1      direct\n",
      "Name: Security_Type, dtype: object\n",
      "0    0\n",
      "1    1\n",
      "Name: Status, dtype: int64\n",
      "0     <NA>\n",
      "1      5.0\n",
      "2      6.0\n",
      "3      7.0\n",
      "4      8.0\n",
      "5      9.0\n",
      "6     10.0\n",
      "7     11.0\n",
      "8     12.0\n",
      "9     13.0\n",
      "10    14.0\n",
      "11    15.0\n",
      "12    16.0\n",
      "13    17.0\n",
      "14    18.0\n",
      "15    19.0\n",
      "16    20.0\n",
      "17    21.0\n",
      "18    22.0\n",
      "19    23.0\n",
      "20    24.0\n",
      "21    25.0\n",
      "22    26.0\n",
      "23    27.0\n",
      "24    28.0\n",
      "25    29.0\n",
      "26    30.0\n",
      "27    31.0\n",
      "28    32.0\n",
      "29    33.0\n",
      "30    34.0\n",
      "31    35.0\n",
      "32    36.0\n",
      "33    37.0\n",
      "34    38.0\n",
      "35    39.0\n",
      "36    40.0\n",
      "37    41.0\n",
      "38    42.0\n",
      "39    43.0\n",
      "40    44.0\n",
      "41    45.0\n",
      "42    46.0\n",
      "43    47.0\n",
      "44    48.0\n",
      "45    49.0\n",
      "46    50.0\n",
      "47    51.0\n",
      "48    52.0\n",
      "49    53.0\n",
      "50    54.0\n",
      "51    55.0\n",
      "52    56.0\n",
      "53    57.0\n",
      "54    58.0\n",
      "55    59.0\n",
      "56    60.0\n",
      "57    61.0\n",
      "Name: dtir1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for column in dataset.columns:\n",
    "    print (dataset.loc[:, column].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e37112",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "1. There are lot of feature vectors with string categorical data, we need to convert those to integer data i.e. assigning a number to each category.\n",
    "2. Once categorical features are encoded then we can change datatype of that feature to integer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38364ed0",
   "metadata": {},
   "source": [
    "### We have lots of 34 features in this dataset checking for categorical feature vectors can take time.\n",
    "### We will automate this in the following manner:\n",
    "1. We can try to find columns which has categorical values with based on how many number of unique values that feature has. <br/>\n",
    "For Ex: ```loan_limit``` feature is a categoriacal vector and has <b>2 unique</b> categories <b>cf</b> and <b>ncf</b>.\n",
    "2. Like the example above we can for all the categorical feature vectors, if a vector has < x number of unique values then we can categorize it as categorical feature.\n",
    "3. Here in the following cell we have ```n_unique_values``` variable which we will set to <b>10</b>. Which means if any vector has < 10 unique values then it's categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5c2221c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. loan_limit has 3 unique values, which are [None, 'cf', 'ncf']\n",
      "2. Gender has 4 unique values, which are ['Female', 'Joint', 'Male', 'Sex Not Available']\n",
      "3. approv_in_adv has 3 unique values, which are [None, 'nopre', 'pre']\n",
      "4. loan_type has 3 unique values, which are ['type1', 'type2', 'type3']\n",
      "5. loan_purpose has 5 unique values, which are [None, 'p1', 'p2', 'p3', 'p4']\n",
      "6. Credit_Worthiness has 2 unique values, which are ['l1', 'l2']\n",
      "7. open_credit has 2 unique values, which are ['nopc', 'opc']\n",
      "8. business_or_commercial has 2 unique values, which are ['b/c', 'nob/c']\n",
      "14. Neg_ammortization has 3 unique values, which are [None, 'neg_amm', 'not_neg']\n",
      "15. interest_only has 2 unique values, which are ['int_only', 'not_int']\n",
      "16. lump_sum_payment has 2 unique values, which are ['lpsm', 'not_lpsm']\n",
      "18. construction_type has 2 unique values, which are ['mh', 'sb']\n",
      "19. occupancy_type has 3 unique values, which are ['ir', 'pr', 'sr']\n",
      "20. Secured_by has 2 unique values, which are ['home', 'land']\n",
      "21. total_units has 4 unique values, which are ['1U', '2U', '3U', '4U']\n",
      "23. credit_type has 4 unique values, which are ['CIB', 'CRIF', 'EQUI', 'EXP']\n",
      "25. co-applicant_credit_type has 2 unique values, which are ['CIB', 'EXP']\n",
      "26. age has 8 unique values, which are [None, '25-34', '35-44', '45-54', '55-64', '65-74', '<25', '>74']\n",
      "27. submission_of_application has 3 unique values, which are [None, 'not_inst', 'to_inst']\n",
      "29. Region has 4 unique values, which are ['North', 'North-East', 'central', 'south']\n",
      "30. Security_Type has 2 unique values, which are ['Indriect', 'direct']\n",
      "31. Status has 2 unique values, which are [0, 1]\n"
     ]
    }
   ],
   "source": [
    "n_unique_values = 10\n",
    "all_columns = dataset.columns.to_list()\n",
    "categorical_columns = []\n",
    "\n",
    "for i, column in enumerate(all_columns):\n",
    "    temp = dataset.loc[:, column].unique().to_arrow().to_pylist()\n",
    "    if len(temp) < n_unique_values:\n",
    "        print (f\"{i+1}. {column} has {len(temp)} unique values, which are {temp}\") \n",
    "        categorical_columns.append(column)\n",
    "del temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faa2294",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "1. We will encode these categorical values before we fix the NA values, but encoder may end up assigning a category to NA values. We need to make sure that it does not happen.\n",
    "2. Feature vector ```Gender``` has a value ```Sex Not Available``` which are essentially NA values.\n",
    "3. Ignore status while performing encode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba7c5e3",
   "metadata": {},
   "source": [
    "### Replacing ```Sex Not Available``` with ```np.nan``` in feature vector ```Gender```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fed251f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Gender\"] = dataset[\"Gender\"].replace(\"Sex Not Available\", np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa8d0ba",
   "metadata": {},
   "source": [
    "### Updating ```cols_with_na_values``` adding ```Gender``` to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "729cb65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dtir1', 'approv_in_adv', 'age', 'Upfront_charges', 'Interest_rate_spread', 'loan_limit', 'income', 'loan_purpose', 'property_value', 'Neg_ammortization', 'rate_of_interest', 'LTV', 'submission_of_application', 'term', 'Gender']\n"
     ]
    }
   ],
   "source": [
    "if \"Gender\" not in cols_with_na_values:\n",
    "    cols_with_na_values.append(\"Gender\")\n",
    "    print (cols_with_na_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f825597",
   "metadata": {},
   "source": [
    "## Encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ab26db9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "na_before = dataset[categorical_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c8557a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bf2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder(verbose=True)\n",
    "for column in categorical_columns:\n",
    "    # Retriving not null data for each feature vector \n",
    "    temp = dataset.loc[ dataset[column].notnull(), column ] \n",
    "    # Replacing the same not null data with an integer for each category in each column\n",
    "    dataset.loc[ dataset[column].notnull(), [column,] ] = le.fit_transform(temp)\n",
    "    \n",
    "del le, temp\n",
    "dataset[categorical_columns].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a23c4",
   "metadata": {},
   "source": [
    "All the categorical data has been transformed to integer categories and as we can see ```Gender``` vector has ```<NA>``` value as well. Just to be sure we can check that all the categorical fearures still has the ```<NA>``` values and those are not replaced with a category in any feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc9b21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "na_before == dataset[categorical_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4e8456",
   "metadata": {},
   "source": [
    "We still have the same number of ```<NA>``` values in all feature vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5db27c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97db10a4",
   "metadata": {},
   "source": [
    "### Now, let's change the datatype of each column with object datatype to int or float as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f23aa38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For the each column in the dataset\n",
    "for column in dataset.columns:\n",
    "    i = 0\n",
    "    column = str(column)\n",
    "    # Checking if datatype of the feature ```column``` is object\n",
    "    if dataset[column].dtype == 'object' or dataset[column].dtype == np.float64 or dataset[column].dtype == np.int64:\n",
    "        # Running a loop until we get non-NA type value for feature ```column```\n",
    "        while True:\n",
    "            # Getting the ith value from ```column``` feature\n",
    "            temp = dataset.loc[i, column]\n",
    "            i += 1\n",
    "            # Checking if the values is of type NA\n",
    "            if type(temp) != pd._libs.missing.NAType:\n",
    "                isFloat = False\n",
    "                # Trying to convert the value to float \n",
    "                try:\n",
    "                    float(temp)\n",
    "                    isFloat = True\n",
    "                except:\n",
    "                    isFloat = False\n",
    "                # Checking if the type of data is string if yes, checking if it's numberic\n",
    "                # In case of float value ```isnumeric()``` will return ```False```\n",
    "                if type(temp) == np.int64 or (type(temp) == str and temp.isnumeric()):\n",
    "                    # Converting the datatype of the feature ```column``` to integer\n",
    "                    print (f\"Coverting feature {column} type from {dataset[column].dtype} to np.int32\")\n",
    "                    dataset[column] = dataset[column].astype(np.int32, copy=False)\n",
    "                elif isFloat or type(temp) == np.float64:\n",
    "                    # Converting the datatype of the feature ```column``` to np.float32\n",
    "                    print (f\"Coverting feature {column} type from {dataset[column].dtype} to np.float32\")\n",
    "                    dataset[column] = dataset[column].astype(np.float32, copy=False)\n",
    "                else:\n",
    "                    # In case type of value is neither integer or float feature ```column``` type as it is\n",
    "                    print (f\" ** Leaving {column} as it is {dataset[column].dtype} ** \")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be8cf6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b2fa4b",
   "metadata": {},
   "source": [
    "### Now that we have the correct datatype for all the feature vectors we can check the relation among all the feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913d67b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_map = dataset.to_pandas().corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a3661",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 20), dpi=100) \n",
    "_ = sns.heatmap(corr_map, annot=True, annot_kws={\"fontsize\":12}, linecolor='white', \\\n",
    "            linewidth=1, fmt='.3f', cmap=\"viridis\", cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e401ba",
   "metadata": {},
   "source": [
    "## Understanding relationship score\n",
    "<pre>\n",
    "Correlation value ranging > 0.7 and 1.0, is highly positive/negetive relationship score,\n",
    "       value > 0.5 and value <= 0.7, is moderate positive/negetive relationship score,\n",
    "       value > 0.0 and value <= 0.5, is low positive/negetive relationship score, and\n",
    "       value == 0.0, is no relationship at all.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604c0836",
   "metadata": {},
   "source": [
    "### This big heatmap can be difficult to read, run the cell below to to create a dictionary and categorise low, moderate and high correlation which another feature.\n",
    "### The code below checks only for one feature's relation with another one feature, there is a possibility that a feature vector may be multicoliniarity please check the heatmap for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa794b0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr_label = np.array([\"high\", \"moderate\", \"low\", \"no\"])\n",
    "pos_neg_corr_label = np.array([\"positive\", \"negative\"])\n",
    "\n",
    "correlations = {}\n",
    "for column in cols_with_na_values:\n",
    "    dict_ = corr_map[column].drop(labels=column).to_dict()\n",
    "    values = list(dict_.values())\n",
    "    keys = list(dict_.keys())\n",
    "    value = max( [abs(min(values)), abs(max(values))] )\n",
    "    relation = ([\n",
    "        value > 0.7 and value <= 1.0, \n",
    "        value > 0.5 and value <= 0.7,\n",
    "        value > 0.0 and value <= 0.5, \n",
    "        value == 0.0,\n",
    "    ]) \n",
    "    try:\n",
    "        key = keys[values.index(value)]\n",
    "    except ValueError:\n",
    "        value *= -1\n",
    "        key = keys[values.index(value)]    \n",
    "    cor_label = corr_label[relation][0]\n",
    "    if cor_label in correlations.keys():\n",
    "        temp = correlations.get(cor_label)\n",
    "        temp[column] = [value, key]\n",
    "        correlations[cor_label].update(temp)\n",
    "    else:\n",
    "        correlations[cor_label] = {column: [value, key]}\n",
    "    print (f\"{column} has {corr_label[relation][0]} {pos_neg_corr_label[[value >= 0, value < 0]][0]} \" +\\\n",
    "    f\"correlation with {key} as {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9179437",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7019e5e",
   "metadata": {},
   "source": [
    "### Given that we have significant relationship with another feature column and all the NaN values of feature X are present in feature vector y, we can make a model to predict missing values\n",
    "For Ex: Finding the total missing values in the feature vector ```DTIR1``` is ```24121``` and ```DTIR1``` feature has ```-0.27``` relation with feature vector ```INCOME```, out of those ```24121``` missing values in ```DTIR1```, ```15081``` are present in ```INCOME``` feature vector and ```9040``` are null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24675bb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cor in correlations.keys():\n",
    "    print ('\\n', '-'*10, 'Correlation:', cor.upper(), '-'*10, '\\n')\n",
    "    for sub in correlations.get(cor):\n",
    "        temp = correlations.get(cor).get(sub)\n",
    "        temp_data = dataset[[sub,temp[1]]]\n",
    "        final = temp_data[ temp_data[sub].isnull() ][temp[1]].isnull().sum()\n",
    "        print (f\"{sub.upper()} vs. {temp[1].upper()}: Corr: {temp[0]:.2f}\")\n",
    "        print (f\"Out of {temp_data[sub].isnull().sum()} nan values in feature {sub.upper()}, \" +\\\n",
    "               f\"{temp_data[sub].isnull().sum()-final} in {temp[1].upper()} are present and {final} are absent\\n\")\n",
    "del temp, temp_data, final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b0440",
   "metadata": {},
   "source": [
    "### As we can see that feature vector ```PROPERTY_VALUE``` is linearaly related with feature vector ```LOAN_AMOUNT``` and all the values which are null in ```PROPERTY_VALUE``` are present in feature vector ```LOAN_AMOUNT```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7ba423",
   "metadata": {},
   "source": [
    "Let's try plotting the data to see how it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff2a2e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=100)\n",
    "_ = sns.scatterplot(x=\"loan_amount\", y=\"property_value\", data=dataset.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a6ecfc",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "\n",
    "1. From the graph we can see there is linear relationship between two features but it's not as much that linear regression can fit the data well.\n",
    "2. Though LinearRegression cannot fit the data well, we will try with LinearRegression and RandomForestRegressor.\n",
    "3. We can see in the graph that LinearRegression is not going to be able to fit to this data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907fa517",
   "metadata": {},
   "source": [
    "### Model building for NA Values in \"loan_amount\" feature using \"property_value\" feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd604f93",
   "metadata": {},
   "source": [
    "### StandardScaling the feature vectors X (loan_amount) and y (property_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b57d0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cde4908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = dataset.loc[dataset[\"property_value\"].notnull(), [\"property_value\", \"loan_amount\"]]\n",
    "\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "data = scaler.fit_transform(data)\n",
    "X = data.iloc[:,1]\n",
    "y = data.iloc[:,0]\n",
    "\n",
    "if X.dtype != np.float32:\n",
    "    X = X.astype(np.float32)\n",
    "    print (f\"Converting X to {X.dtype}\")\n",
    "if y.dtype != np.float32:\n",
    "    y = y.astype(np.float32)\n",
    "    print (f\"Converting y to {y.dtype}\")\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ba7484",
   "metadata": {},
   "source": [
    "### Model Creation\n",
    "Applying LinearRegression and RandomForest algoritms to X = \"property_value\" and y = \"loan_amount\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6192d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.linear_model import LinearRegression\n",
    "from cuml.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d917c690",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (f\"Number of samples for training: {X.shape} the model\")\n",
    "\n",
    "params = {\n",
    "#     'eig' solver does not support training data with 1 column currently. rapids v22.02\n",
    "    'algorithm': ['svd', 'qr', 'svd-qr', 'svd-jacobi'],\n",
    "}\n",
    "\n",
    "total_n_perm = len(ParameterGrid(param_grid=params))\n",
    "print (f\"Total number of permutations need to run by GridSearchCV is {total_n_perm}\")\n",
    "\n",
    "gcv = GridSearchCV(LinearRegression(), params, cv = 5, return_train_score=True)\n",
    "gcv.fit(X.copy(), y.copy())\n",
    "\n",
    "print ('best solver:', gcv.best_estimator_)\n",
    "print ('best params:', gcv.best_params_)\n",
    "print ('best score:', gcv.best_score_)\n",
    "print ('estimator:', gcv.estimator)\n",
    "print ('train score:', gcv.best_estimator_.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344b4550",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=100)\n",
    "\n",
    "model_lr = gcv.best_estimator_\n",
    "\n",
    "_ = plt.scatter(X.to_numpy(), y.to_numpy(), color='red', marker='+')\n",
    "_ = plt.plot(X.to_numpy(), model_lr.predict(X).to_numpy(), color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0f904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0526b24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (f\"Number of samples for training: {X.shape} the model\")\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [x for x in range(10, 101, 5)],\n",
    "}\n",
    "\n",
    "total_n_perm = len(ParameterGrid(param_grid=params))\n",
    "print (f\"Total number of permutations need to run by GridSearchCV is {total_n_perm}\")\n",
    "\n",
    "gcv = GridSearchCV(RandomForestRegressor(), params, cv=5, return_train_score=True)\n",
    "gcv.fit(X.copy(), y.copy())\n",
    "\n",
    "print ('best solver:', gcv.best_estimator_)\n",
    "print ('best params:', gcv.best_params_)\n",
    "print ('best score:', gcv.best_score_)\n",
    "print ('estimator:', gcv.estimator)\n",
    "print ('train score:', gcv.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7020d5f2",
   "metadata": {},
   "source": [
    "### Visualizing all 5 cross-validation results with DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82bf726",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cudf.DataFrame(gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b176bb",
   "metadata": {},
   "source": [
    "### The accuracy we are getting with LinearRegression or RandomForestRegression is not enough for us to use the trained model.\n",
    "### We tried building the model with 2 feature vectors which had highest correlation score, with that we got ~0.54 accuracy, this makes it clear that we are not going get good model with correlation score any lower than that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e48ee13",
   "metadata": {},
   "source": [
    "### We can predict missing values in the feature vector ```loan_amount``` with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b207d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_vals = dataset.loc[dataset[\"property_value\"].isnull(), [\"loan_amount\",]].copy()\n",
    "missing_vals = scaler.fit_transform(missing_vals)\n",
    "\n",
    "model_rf = gcv.best_estimator_\n",
    "\n",
    "model_rf.predict(missing_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef2d6e",
   "metadata": {},
   "source": [
    "### Since, building a model is not going to work because of low correlation score, we will replace the missing values with mode value of each class status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506054f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_vals = dataset[\"Status\"].unique().values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45953ff8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for status in status_vals:\n",
    "    if status is np.nan:\n",
    "        print (\"Status value is NaN\")\n",
    "        continue\n",
    "    total_samples = dataset[dataset[\"Status\"] == status].shape[0]\n",
    "    max_null_vals = np.max(dataset[dataset[\"Status\"] == status].isnull().sum().values)\n",
    "    msg = f\"Total samples with status as {status} are {total_samples} out of those {max_null_vals} \" +\\\n",
    "    \"samples have null values\"\n",
    "    print (msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf673277",
   "metadata": {},
   "source": [
    "### Replacing null values in categorical features with mode of the feature status i.e. 0 or 1 and continous features with median value of that feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4302f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in dataset.columns:\n",
    "    if dataset[col].isnull().sum() > 0:\n",
    "        for status in status_vals:\n",
    "            if col in categorical_columns:\n",
    "                vec_type_value = (\"categorical\", \"mode\")\n",
    "                value = dataset.loc[dataset[\"Status\"] == status, col].mode().to_arrow().to_pylist()[0]\n",
    "            else:\n",
    "                vec_type_value = (\"continuous\", \"median\")\n",
    "                value = dataset.loc[dataset[\"Status\"] == status, col].median()\n",
    "            print (f\"Replacing {vec_type_value[0]} feature vector \\\"{col}\\\" where status: {status} with \" +\\\n",
    "                   f\"{vec_type_value[1]}: {value:.3f}\")\n",
    "            if str(np.nan) != str(value):\n",
    "                dataset.loc[dataset[\"Status\"] == status, col] = dataset.loc[dataset[\"Status\"] == status, col].fillna(value=value)\n",
    "            else:\n",
    "                print (f\"--->> Skipping {vec_type_value[0]} feature vector \\\"{col}\\\" as {vec_type_value[1]} is {np.nan}\")\n",
    "    else:\n",
    "        print (f\"** There are no null values in feature vector \\\"{col}\\\" **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6fc518",
   "metadata": {},
   "source": [
    "### Checking if any null values are still left in any feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf1a1e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b63989",
   "metadata": {},
   "source": [
    "### There is 1 feature vectors ```Interest_rate_spread``` which still has large number of null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6452095f",
   "metadata": {},
   "source": [
    "Let's check the relationship of this features with dependent feature \"Status\", if there is no significant correlation of these features with \"Status\" feature then we can drop it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac894b9",
   "metadata": {},
   "source": [
    "We can use code in the code below or you can check heatmap we generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7eae8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset[[\"Interest_rate_spread\",]].to_pandas().corrwith(dataset[\"Status\"].to_pandas(), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15d7951",
   "metadata": {},
   "source": [
    "### As we can see that there is no significant correlation of the feature vectors ```Interest_rate_spread``` with dependent feature vector ```Status```, we can simply drop these feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5551f4e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.drop([\"Interest_rate_spread\",], axis=1, inplace=True)\n",
    "print (dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f1ce7e",
   "metadata": {},
   "source": [
    "## train_test_split dataset\n",
    "* We will split the data into 80, 20 split ratio for train and test dataset\n",
    "* We will fit StandardScaler on X_train data and using that scaler object, we will transform X_train and X_test both to avoid data leak problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec25a33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = dataset[\"Status\"].copy()\n",
    "dt2 = dataset.drop([\"Status\", ], axis=1, inplace=False).copy()\n",
    "X = dt2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 99, test_size = 0.2, shuffle = False, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9ccccb",
   "metadata": {},
   "source": [
    "Resetting index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7234f3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(inplace=True, drop=True)\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "y_train.reset_index(inplace = True, drop=True)\n",
    "y_test.reset_index(inplace = True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b751015f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05903f4",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "#### Before we apply any algorithm to our data we need to scale the data and scaling the entire dataset can lead to \"data leak\" problem.\n",
    "#### To avoid this we will split the data to train and test first and we will fit the scaler object to train data and use the same scaler object to transform (apply) both train and test dataset.\n",
    "#### We will use the same train and test split through out all the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bdfa52",
   "metadata": {},
   "source": [
    "## Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf593e9",
   "metadata": {},
   "source": [
    "Continuous feature vectors can range between very nigh to very low values, thus not following gaussian distribution and this can affect our model in significant way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da3df79",
   "metadata": {},
   "source": [
    "We need perform feature scaling to convert feature vectors to standard normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed774e",
   "metadata": {},
   "source": [
    "### But first we need to find what are the continuous feature vectors in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b539a747",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "continuous_columns = list(set(dataset.columns.values.tolist()) - set(categorical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119206f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"In our dataset we have {len(continuous_columns)} continuous feature vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fc30bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset[continuous_columns].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d20633",
   "metadata": {},
   "source": [
    "### Checking the range of values in these feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079272a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scale_columns = []\n",
    "for column in dataset[continuous_columns].columns:\n",
    "    min_ = int(dataset[column].min())\n",
    "    max_ = int(dataset[column].max())\n",
    "    print (f\"Feature {column} ranges between {min_} and {max_}\")\n",
    "    scale_columns.append(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2aa24b",
   "metadata": {},
   "source": [
    "## Applying StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714bedaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "if len(scale_columns) > 1:\n",
    "    columns = {0: scale_columns[0], 1: scale_columns[1], 2: scale_columns[2], 3: scale_columns[3], \\\n",
    "               4: scale_columns[4], 5: scale_columns[5], 6: scale_columns[6], 7: scale_columns[7],\n",
    "              }\n",
    "    scaler = scaler.fit(X_train[scale_columns])\n",
    "    X_train[scale_columns] = scaler.transform(X_train[scale_columns].copy()).rename(columns = columns)\n",
    "    X_test[scale_columns] = scaler.transform(X_test[scale_columns].copy()).rename(columns = columns)\n",
    "elif len(scale_columns) == 1:\n",
    "    scaler.fit(X_train[scale_columns].values.reshape(-1, 1))\n",
    "    X_train[scale_columns] = scaler.transform(X_train[scale_columns].reshape(-1, 1).copy()).reshape(1, -1)\n",
    "    X_test[scale_columns] = scaler.transform(X_test[scale_columns].reshape(-1, 1).copy()).reshape(1, -1)\n",
    "\n",
    "del scaler\n",
    "X_train[scale_columns].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f622f9",
   "metadata": {},
   "source": [
    "## Checking datatype of the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0cd6ad",
   "metadata": {},
   "source": [
    "StandardScaling can change the datatype of the feature vectors to float64 / int64 from float32 / int32 and NVIDIA libraries are designed to only work with float32 / int32 types of variables. Let's check the datatype of all vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee49a96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dfe1c9",
   "metadata": {},
   "source": [
    "As we can see the all the feature vectors passed to StandardScaler are converted to int64 / float64 vectors, let's change those back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068da24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dtype(dataset):\n",
    "    for col in dataset.columns:\n",
    "        if dataset[col].dtype == np.int64:\n",
    "            dataset[col] = dataset[col].astype(np.int32)\n",
    "        elif dataset[col].dtype == np.float64:\n",
    "            dataset[col] = dataset[col].astype(np.float32)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b949c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train[scale_columns] = change_dtype(X_train[scale_columns])\n",
    "if y_train.dtype == np.int64:\n",
    "    y_train = y_train.astype(np.int32)\n",
    "    print (f\"Changing y_train to {y_train.dtype}\")\n",
    "\n",
    "X_test[scale_columns] = change_dtype(X_test[scale_columns])\n",
    "if y_test.dtype == np.int64:\n",
    "    y_test = y_test.astype(np.int32)\n",
    "    print (f\"Changing y_train to {y_test.dtype}\")\n",
    "\n",
    "print (X_train.info())\n",
    "print (X_test.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8006b6da",
   "metadata": {},
   "source": [
    "## Checking for unbalanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23df9e4",
   "metadata": {},
   "source": [
    "First let's see how much data we have in our test and test split, make sure we have enough number of samples to train the model properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3471944a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print (f\"Training dataset shape X_train.shape: {X_train.shape}\")\n",
    "print (f\"Training dataset shape y_test.shape: {y_train.shape}\")\n",
    "print ()\n",
    "print (f\"Testing dataset shape X_test.shape: {X_test.shape}\")\n",
    "print (f\"Testing dataset shape y_test.shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b1657",
   "metadata": {},
   "source": [
    "### Let's also check that we have enough number training and testing samples for all classes (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40872cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in dataset[\"Status\"].unique().values.tolist():\n",
    "    print (f\"Training samples for class {status}: {X_train[y_train == status].shape[0]}\")    \n",
    "    print (f\"Testing samples for class {status}: {X_test[y_test == status].shape[0]}\")\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ccbfd3",
   "metadata": {},
   "source": [
    "## Common functions\n",
    "Following common functions we will be using more than once in the upcoming code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a13db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_adjusted(r2, n, p):\n",
    "    r2_adj = 1 - ((1 - r2) * ((n - 1) / (n - p -1)))\n",
    "    return r2_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11898058",
   "metadata": {},
   "source": [
    "### Now, let's create final model we will drop few different ML models here and will check which one is working better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d3fef",
   "metadata": {},
   "source": [
    "## RandomForestRegressor Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6130e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e4c18a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf_reg = rf.fit(X_train.copy(), y_train.copy())\n",
    "\n",
    "print (f\"Train score: {rf_reg.score(X_train.copy(), y_train.copy())}\")\n",
    "print (f\"Test score: {rf_reg.score(X_test.copy(), y_test.copy())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1404f587",
   "metadata": {},
   "source": [
    "### To check how many samples we predicted correctly, let's check confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dd7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe6eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_reg.predict(X_test.copy())\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88479ad4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(cm.get(), display_labels=rf_reg.classes_.to_numpy()).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f8d6f9",
   "metadata": {},
   "source": [
    "### Let's check the classification_report, it'll show us lots of necessary model evaluation details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4310635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc89acb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print (classification_report(y_test.to_numpy(), y_pred.to_numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72538fa7",
   "metadata": {},
   "source": [
    "### Let's check the R2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456a7a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d15b008",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r2 = r2_score(y_test.to_numpy(), y_pred.to_numpy())\n",
    "print (f\"R-suared score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57786b7a",
   "metadata": {},
   "source": [
    "### R2 adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d07f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2 squared adjusted\n",
    "n, p = X_test.shape\n",
    "r2_adj = r2_adjusted(r2, n, p)\n",
    "print (f\"R-squared adjusted score: {r2_adj}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b7b648",
   "metadata": {},
   "source": [
    "## MLPClassifier Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46a79d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea803b39",
   "metadata": {},
   "source": [
    "### Note: The following cell will take few hours to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac253db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print (f\"Number of samples for training: {X_train.shape} the model\")\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# batch_size = 1000\n",
    "# max_iter = X_train.shape[0] // batch_size # number of epochs\n",
    "\n",
    "# params = {\n",
    "#     'hidden_layer_sizes': [\n",
    "#         (20, 20), (25, 25),\n",
    "#     ],\n",
    "#     'activation': ['relu', ],\n",
    "#     'learning_rate': [\n",
    "#         'adaptive','constant',\n",
    "#     ],\n",
    "#     'learning_rate_init': [\n",
    "#         1e-2, 3e-2, 1e-3, 3e-3, 1e-4, 3e-4,\n",
    "#     ],\n",
    "#     'alpha': [\n",
    "#         1e-5, 3e-5, 1e-6, 3e-6,\n",
    "#     ],\n",
    "#     'tol': [\n",
    "#         1e-4, 1e-5,\n",
    "#     ],\n",
    "#     'early_stopping': [True,],\n",
    "#     'n_iter_no_change': [10, ],\n",
    "#     'max_iter': [max_iter, ],\n",
    "#     'solver': [\n",
    "#         'lbfgs', 'sgd', 'adam',\n",
    "#     ],\n",
    "#     'batch_size': [batch_size, ],\n",
    "#     'validation_fraction': [0.2, ],\n",
    "#     'shuffle': [True, ],\n",
    "#     'verbose': [True, ],\n",
    "# }\n",
    "\n",
    "# total_n_perm = len(ParameterGrid(param_grid=params))\n",
    "# print (f\"Total number of permutations need to run by GridSearchCV is {total_n_perm}\")\n",
    "\n",
    "# gcv = GridSearchCV(MLPClassifier(), params, cv=skf, return_train_score=True)\n",
    "# gcv.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "\n",
    "# print ('best solver:', gcv.best_estimator_)\n",
    "# print ('best params:', gcv.best_params_)\n",
    "# print ('best score:', gcv.best_score_)\n",
    "# print ('estimator:', gcv.estimator)\n",
    "\n",
    "# mlp = gcv.best_estimator_\n",
    "\n",
    "# print (f\"Train score: {mlp.score(X_train.to_numpy(), y_train.to_numpy())}\")\n",
    "# print (f\"Test score: {mlp.score(X_test.to_numpy(), y_test.to_numpy())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96ba4fc",
   "metadata": {},
   "source": [
    "You can uncomment the following cell in case you want immediate MLPClassifier results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6c763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes = (20, 20),\n",
    "    activation = 'relu',\n",
    "    alpha = 1e-5,\n",
    "    learning_rate = 'adaptive',\n",
    "    learning_rate_init = 1e-3,\n",
    "    tol = 1e-6,\n",
    "    early_stopping = True,\n",
    "    validation_fraction = 2e-1,\n",
    "    n_iter_no_change = 50,\n",
    "    max_iter = 1000,\n",
    "    solver = 'adam', # 'lbfgs', 'sgd'\n",
    ")\n",
    "\n",
    "print (mlp.fit(X_train.to_numpy(), y_train.to_numpy()))\n",
    "\n",
    "print (f\"iterations ran: {mlp.n_iter_}\")\n",
    "print (f\"Train score: {mlp.score(X_train.to_numpy(), y_train.to_numpy())}\")\n",
    "print (f\"Test score: {mlp.score(X_test.to_numpy(), y_test.to_numpy())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b438773a",
   "metadata": {},
   "source": [
    "## Visualizing all 5 cross-validation results with DataFrame.\n",
    "Note: Uncomment the following cell in case you have not used GridSearchCV for MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d048edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cudf.DataFrame(gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a595d100",
   "metadata": {},
   "source": [
    "### To check how many samples we predicted correctly, let's check confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2266cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbd40be",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_test.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a595d10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95efe919",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = ConfusionMatrixDisplay(cm.get(), display_labels=rf_reg.classes_.to_numpy()).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae7abdb",
   "metadata": {},
   "source": [
    "### Let's check the classification_report, it'll show us lots of necessary model evaluation details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce128c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b3c677",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print (classification_report(y_test.to_numpy(), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35918aa9",
   "metadata": {},
   "source": [
    "### Let's check the R2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac8f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1243793",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r2 = r2_score(y_test.to_numpy(), y_pred)\n",
    "print (f\"R-suared score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26780544",
   "metadata": {},
   "source": [
    "### R2 adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2 squared adjusted\n",
    "n, p = X_test.shape\n",
    "r2_dj = r2_adjusted(r2, n, p)\n",
    "print (f\"R-squared adjusted score: {r2_adj}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812397b8",
   "metadata": {},
   "source": [
    "## Model training using ANN (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407816d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0f4886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Dropout\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b899192f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_ann = Sequential(name=\"loan-defaulter-keras-ann\")\n",
    "model_ann.add( InputLayer(input_shape = (X_train.shape[1],), name=\"keras-input-layer\") )\n",
    "model_ann.add( Dense(units = 35, activation = 'leaky_relu', use_bias = True, name = 'second-dense-35-units') )\n",
    "model_ann.add( Dense(units = 35, activation = 'leaky_relu', use_bias = True, name = 'third-dense-35-units') )\n",
    "model_ann.add( Dense(units = 35, activation = 'leaky_relu', use_bias = True, name = 'fourth-dense-35-units') )\n",
    "model_ann.add( Dense(units = 1, activation = 'sigmoid', use_bias = True, name = 'output_sigmoid') )\n",
    "model_ann.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy', 'mse'])\n",
    "model_ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e3b6eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "epochs = X_train.shape[0] // batch_size\n",
    "\n",
    "hist = model_ann.fit(\n",
    "    X_train.to_pandas(), \n",
    "    y_train.to_pandas(),\n",
    "    validation_data = (X_test.to_pandas(), y_test.to_pandas()), \n",
    "    epochs = epochs, \n",
    "    batch_size = batch_size,\n",
    "    verbose = 1,\n",
    "    use_multiprocessing = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c9a015",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7f072",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss, acc, mse = model_ann.evaluate(X_test.to_numpy(), y_test.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ed766",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"Loss: {loss:.3f}, Accuracy: {acc:.3f} and MSE: {mse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd190035",
   "metadata": {},
   "source": [
    "### To check how many samples we predicted correctly, let's check confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc48ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6143d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_ann.predict(X_test.to_numpy()).round().astype(np.int32)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a595d11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95efe920",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = ConfusionMatrixDisplay(cm.get(), display_labels=rf_reg.classes_.to_numpy()).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae7abdc",
   "metadata": {},
   "source": [
    "### Let's check the classification_report, it'll show us lots of necessary model evaluation details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce128c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b3c678",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print (classification_report(y_test.to_numpy(), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35918aa8",
   "metadata": {},
   "source": [
    "### Let's check the R2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac8f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1243792",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r2 = r2_score(y_test.to_numpy(), y_pred)\n",
    "print (f\"R-suared score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26780545",
   "metadata": {},
   "source": [
    "### R2 adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b5139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2 squared adjusted\n",
    "n, p = X_test.shape\n",
    "r2_adj = r2_adjusted(r2, n, p)\n",
    "print (f\"R-squared adjusted score: {r2_adj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e04d3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3)\n",
    "fig.figure = (24, 10)\n",
    "fig.dpi = 100\n",
    "\n",
    "axs[0].plot(hist.history['accuracy'], label=\"accuracy\")\n",
    "axs[0].plot(hist.history['loss'], label=\"loss\")\n",
    "\n",
    "axs[0].set_xlabel(\"epochs\")\n",
    "axs[0].set_ylabel(\"loss\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(hist.history['val_accuracy'], label=\"val accuracy\")\n",
    "axs[1].plot(hist.history['val_loss'], label=\"loss\")\n",
    "\n",
    "axs[1].set_xlabel(\"epochs\")\n",
    "axs[1].set_ylabel(\"val loss\")\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].plot(hist.history['val_mse'], label=\"val mse\")\n",
    "axs[2].legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
